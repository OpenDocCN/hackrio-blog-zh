<html>
<head>
<title>Top Steps To Learn Naive Bayes Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>学习朴素贝叶斯算法的首要步骤</h1>
<blockquote>原文：<a href="https://hackr.io/blog/top-steps-to-learn-naive-bayes-algorithm#0001-01-01">https://hackr.io/blog/top-steps-to-learn-naive-bayes-algorithm#0001-01-01</a></blockquote><div><div class="content">
										<h2 id="what-is-the-naive-bayes-algorithm">什么是朴素贝叶斯算法？</h2>
<p>朴素贝叶斯模型，不管它做出什么样的强假设，由于其简单性和所需的参数分类数量少，在实践中经常被使用。该模型通常用于分类——根据给定实例的证据变量的值，决定该实例最有可能属于哪个类。</p>
<p>一种基于贝叶斯定理的朴素贝叶斯分类器算法，它提供了一种洞察力，即随着新数据的引入，可以调整事件的概率。这是一种概率算法，这意味着它计算给定文本的每个标签的概率，然后输出最高的标签。该算法不是单一的，而是使用统计独立性的不同机器学习算法的集合，易于编写，运行效率比复杂的贝叶斯算法更高。</p>
<h2 id="working-of-naive-bayes-example">朴素贝叶斯的工作:例子</h2>
<h3 id="classification">分类</h3>
<p>假设我们有一个数据集，其中有天气情况，湿度，我们需要确定那天我们是否应该比赛。前景可能是晴天、阴天或雨天，湿度较高或正常。风分为弱风和强风两类。</p>
<h3 id="dataset">资料组</h3>
<table>
<tbody>
<tr>
<td>一天</td>
<td>观点</td>
<td>湿度</td>
<td>风</td>
<td>玩</td>
</tr>
<tr>
<td>D1</td>
<td>快活的</td>
<td>高的</td>
<td>无力的</td>
<td>不</td>
</tr>
<tr>
<td>D2</td>
<td>快活的</td>
<td>高的</td>
<td>强烈的</td>
<td>不</td>
</tr>
<tr>
<td>D3</td>
<td>遮蔽</td>
<td>高的</td>
<td>无力的</td>
<td>是</td>
</tr>
<tr>
<td>D4</td>
<td>雨</td>
<td>高的</td>
<td>无力的</td>
<td>是</td>
</tr>
<tr>
<td>D5</td>
<td>雨</td>
<td>常态</td>
<td>无力的</td>
<td>是</td>
</tr>
<tr>
<td>D6</td>
<td>雨</td>
<td>常态</td>
<td>强烈的</td>
<td>不</td>
</tr>
<tr>
<td>D7</td>
<td>遮蔽</td>
<td>常态</td>
<td>强烈的</td>
<td>是</td>
</tr>
<tr>
<td>D8</td>
<td>快活的</td>
<td>高的</td>
<td>无力的</td>
<td>不</td>
</tr>
<tr>
<td>D9</td>
<td>快活的</td>
<td>常态</td>
<td>无力的</td>
<td>是</td>
</tr>
<tr>
<td>D10</td>
<td>雨</td>
<td>常态</td>
<td>无力的</td>
<td>是</td>
</tr>
<tr>
<td>D11</td>
<td>快活的</td>
<td>常态</td>
<td>强烈的</td>
<td>是</td>
</tr>
<tr>
<td>D12</td>
<td>遮蔽</td>
<td>高的</td>
<td>强烈的</td>
<td>是</td>
</tr>
<tr>
<td>D13</td>
<td>遮蔽</td>
<td>常态</td>
<td>无力的</td>
<td>是</td>
</tr>
<tr>
<td>D14</td>
<td>雨</td>
<td>高的</td>
<td>强烈的</td>
<td>不</td>
</tr>
</tbody>
</table>
<p>数据集每个属性的频率表如下所示:</p>

<p><img src="../Images/6097c33804e629792c8618e103db7d6b.png" alt="Frequency Tables." data-original-src="https://hackr.io/blog/media/frequency-tables.png"/></p>
<p>以下是为每个频率表生成的可能性表:</p>
<p><img src="../Images/953c3881ee42ad65c3e6ff2a3373e093.png" alt="LIkelishood Table" data-original-src="https://hackr.io/blog/media/liklihood.png"/></p>
<p>P(x | c)= P(Sunny | Yes)= 3/10 = 0.3<br/><span>P(x)= P(Sunny)= 5/14 = 0.36<br/></span><span>P(c)= P(Yes)= 10/14 = 0.71</span></p>
<h3 id="attribute-outlook-sunny">属性:展望:阳光明媚</h3>
<p>假设sunny回答“是”的可能性是:</p>
<p>P(c|x) = P(是|晴)= P(晴|是)* P(是)| P(晴)<br/> = 0.3 x 0.71/ 0.36 = 0.591</p>

<p>给sunny“否”的可能性是:</p>
<p>P(c | x)= P(No | Sunny)= P(Sunny | No)* P(No)| P(Sunny)<br/>= 0.4 x 0.36/0.36 = 0.40</p>
<h3 id="attribute-humidity-high">属性:湿度:高</h3>
<p>鉴于湿度较高，回答“是”的可能性为:</p>
<p>P(c|x) = P(是|湿度)= P(湿度|是)* P(是)| P(高)<br/> = 0.33 x 0.6 / 0.36 = 0.42</p>
<p>给定高湿度条件下“否”的可能性为:</p>
<p>P(c|x) = P(无|高)= P(高|无)* P(无)| P(高)。<br/>= 0.8×0.36/0.5 = 0.58</p>
<h3 id="attribute-wind-weak">属性:风:弱</h3>
<p>在弱风的情况下，“是”的可能性为:</p>
<p>P(c|x) = P(是|湿度)= P(湿度|是)* P(是)| P(高)<br/> = 0.67 x 0.64 / 0.57 = 0.75</p>
<p>给定弱风的“否”的可能性为:</p>
<p>P(c|x) = P(无|高)= P(高|无)* P(无)| P(高)<br/> = 0.4 x 0.36/ 0.57 = 0.25</p>
<p>假设我们有一天有以下值:</p>
<p>展望:雨<br/> <span>湿度:高<br/> </span> <span>风力:弱<br/> </span> <span>发挥:？</span></p>
<p>那天“不”的可能性。</p>
<p>P(展望=下雨|无)* P(湿度=高|无)* P(风=弱|无)* P(无)</p>
<p><img src="../Images/450eac5876f7090bce614938bcf915c8.png" data-original-src="https://hackr.io/blog/media/example.png"/></p>
<p>P(是)= 0.0199/(0.0199+0.0166)= 0.55<br/>P(否)= 0.0166 / (0.0199 + 0.0166)= 0.45</p>
<p>因此，该模型预测明天有55%的可能性会有一场比赛。</p>
<p><a class="btn btn-primary btn-call-to-action " href="https://click.linksynergy.com/deeplink?id=jU79Zysihs4&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdata-science-machine-learning-naive-bayes-in-python%2F" target="_blank" rel="noopener">数据科学&amp;机器学习:Python中的朴素贝叶斯</a></p>
<h2 id="steps-involve-naive-bayes-algorithm">步骤涉及朴素贝叶斯算法</h2>
<p><strong>示例:PIMA糖尿病测试</strong></p>
<p>该问题包括对患者记录的医疗细节的768次观察，描述了从患者获取的即时测量，例如他们的年龄、怀孕次数、血型。所有属性都是数字，单位因属性而异。每个记录都有一个类值，指示患者在五年内是否患有糖尿病。整个过程可以归结为五个步骤:</p>
<h4 id="step-1-handling-data">步骤1:处理数据</h4>
<p>数据从<a href="https://github.com/dtroupe18/SimpleNaiveBayes/blob/master/pima-indians-diabetes.data.csv" target="_blank" rel="noopener"> CSV文件</a>加载，并传播到培训和测试资产中。</p>
<h4 id="step-2-summarizing-the-data">步骤2:汇总数据</h4>
<p>总结训练数据集中的属性，以计算概率并进行预测。</p>
<h4 id="step-3-making-a-prediction">第三步:做预测</h4>
<p>使用数据集的摘要来进行单个预测，从而进行特定的预测。</p>
<h4 id="step-4-making-all-the-predictions">第四步:做所有的预测</h4>
<p>给定一个测试数据集和一个汇总数据集，生成预测。</p>
<h4 id="step-5-evaluate-accuracy">第五步:评估准确性</h4>
<p>测试数据集预测模型的准确性，以所有预测中正确的百分比表示。</p>
<h4 id="step-6-tying-all-together">第六步:把所有东西绑在一起</h4>
<p>最后，我们将所有步骤结合在一起，形成我们自己的朴素贝叶斯分类器模型。</p>

<h3 id="code">密码</h3>
<pre>import csv<br/>import random<br/>import math<br/>import numpy as np<br/><br/>def load_csv(filename):<br/> """<br/> :param filename: name of csv file<br/> :return: data set as a 2 dimensional list where each row in a list<br/> """<br/> lines = csv.reader(open(filename, 'r'))<br/> dataset = list(lines)<br/> for i in range(len(dataset)):<br/> dataset[i] = [float(x) for x in dataset[I]]<br/> return dataset<br/> # data = load_csv('pima-indians-diabetes.data.csv')<br/> # print(data)<br/><br/>def split_dataset(dataset, ratio):<br/> """<br/> split dataset into training and testing<br/> :param dataset: Two dimensional list<br/> :param ratio: Percentage of data to go into the training set<br/> :return: Training set and testing set<br/> """<br/> size_of_training_set = int(len(dataset) * ratio)<br/> train_set = []<br/> test_set = list(dataset)<br/> while len(train_set) &lt; size_of_training_set:<br/> index = random.randrange(len(test_set))<br/> train_set.append(test_set.pop(index))<br/> return [train_set, test_set]<br/><br/># training_set, testing_set = split_dataset(data, 0.67)<br/># print(training_set)<br/># print(testing_set)<br/><br/>def separate_by_label(dataset):<br/> """<br/> :param dataset: two dimensional list of data values<br/> :return: dictionary where labels are keys and<br/> values are the data points with that label<br/> """<br/> separated = {}<br/> for x in range(len(dataset)):<br/> row = dataset[x]<br/> if row[-1] not in separated:<br/> separated[row[-1]] = []<br/> separated[row[-1]].append(row)<br/><br/>return separated<br/><br/># separated = separate_by_label(data)<br/># print(separated)<br/># print(separated[1])<br/># print(separated[0])<br/><br/>def calc_mean(last):<br/><br/> return sum(lst) / float(len(last))<br/><br/>def calc_standard_deviation(last):<br/> avg = calc_mean(last)<br/> variance = sum([pow(x - avg, 2) for x in lst]) / float(len(lst) - 1)<br/> return math.sqrt(variance)<br/><br/># numbers = [1, 2, 3, 4, 5]<br/># print(calc_mean(numbers))<br/># print(calc_standard_deviation(numbers))<br/><br/>def summarize_data(last):<br/><br/> """<br/> Calculate the mean and standard deviation for each attribute<br/> :param lst: list<br/> :return: list with mean and standard deviation for each attribute<br/> """<br/><br/> summaries = [(calc_mean(attribute), calc_standard_deviation(attribute)) <br/> for attribute in zip(*lst)]<br/> del summaries[-1]<br/><br/> return summaries<br/><br/># summarize_me = [[1, 20, 0], [2, 21, 1], [3, 22, 0]]<br/># print(summarize_data(summarize_me))<br/><br/>def summarize_by_label(data):<br/><br/> """<br/> Method to summarize the attributes for each label<br/> :param data:<br/> :return: dict label: [(atr mean, atr stdv), (atr mean, atr stdv)....]<br/> """<br/> separated_data = separate_by_label(data)<br/> summaries = {}<br/> for label, instances in separated_data.items():<br/> summaries[label] = summarize_data(instances)<br/> return summaries<br/><br/># fake_data = [[1, 20, 1], [2, 21, 0], [3, 22, 1], [4,22,0]]<br/># fake_summary = summarize_by_label(fake_data)<br/><br/>def calc_probability(x, mean, standard_deviation):<br/> """<br/> :param x: value<br/> :param mean: average<br/> :param standard_deviation: standard deviation<br/> :return: probability of that value given a normal distribution<br/> """<br/> # e ^ -(y - mean)^2 / (2 * (standard deviation)^2)<br/> exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(standard_deviation, 2))))<br/> # ( 1 / sqrt(2π) ^ exponent<br/> return (1 / (math.sqrt(2 * math.pi) * standard_deviation)) * exponent<br/> <br/># x = 57<br/># mean = 50<br/># stand_dev = 5<br/># print(calc_probability(x, mean, stand_dev))<br/><br/>def calc_label_probabilities(summaries, input_vector):<br/> """<br/> the probability of a given data instance is calculated by multiplying together<br/> the attribute probabilities for each class. The result is a map of class values<br/> to probabilities.<br/> :param summaries:<br/> :param input_vector:<br/> :return: dict<br/> """<br/> probabilities = {}<br/> for label, label_summaries in summaries.items():<br/> probabilities[label] = 1<br/> for i in range(len(label_summaries)):<br/> mean, standard_dev = label_summaries[I]<br/> x = input_vector[I]<br/> probabilities[label] *= calc_probability(x, mean, standard_dev)<br/><br/>return probabilities<br/><br/># fake_input_vec = [1.1, 2.3]<br/># fake_probabilities = calc_label_probabilities(fake_summary, fake_input_vec)<br/># print(fake_probabilities)<br/><br/>def predict(summaries, input_vector):<br/><br/> """<br/> Calculate the probability of a data instance belonging<br/> to each label. We look for the largest probability and return<br/> the associated class.<br/> :param summaries:<br/> :param input_vector:<br/> :return:<br/> """<br/> probabilities = calc_label_probabilities(summaries, input_vector)<br/> best_label, best_prob = None, -1<br/> for label, probability in probabilities.items():<br/> if best_label is None or probability &gt; best_prob:<br/> best_prob = probability<br/> best_label = label<br/> return best_label<br/><br/># summaries = {'A': [(1, 0.5)], 'B': [(20, 5.0)]}<br/># inputVector = 1.1<br/># print(predict(summaries, inputVector))<br/><br/>def get_predictions(summaries, test_set):<br/><br/> """<br/> Make predictions for each data instance in our<br/> test dataset<br/> """<br/> predictions = []<br/> for i in range(len(test_set)):<br/> result = predict(summaries, test_set[i])<br/> predictions.append(result)<br/><br/> return predictions<br/><br/># summaries = {'A': [(1, 0.5)], 'B': [(20, 5.0)]}<br/># testSet = [1.1, 19.1]<br/># predictions = get_predictions(summaries, testSet)<br/># print(predictions)<br/><br/>def get_accuracy(test_set, predictions):<br/><br/> """<br/> Compare predictions to class labels in the test dataset<br/> and get our classification accuracy<br/> """<br/> correct = 0<br/> for i in range(len(test_set)):<br/> if test_set[i][-1] == predictions[i]:<br/> correct += 1<br/><br/> return (correct / float(len(test_set))) * 100<br/><br/># fake_testSet = [[1, 1, 1, 'a'], [2, 2, 2, 'a'], [3, 3, 3, 'b']]<br/># fake_predictions = ['a', 'a', 'a']<br/># fake_accuracy = get_accuracy(fake_testSet, fake_predictions)<br/># print(fake_accuracy)<br/><br/>def main(filename, split_ratio):<br/><br/> data = load_csv(filename)<br/> training_set, testing_set = split_dataset(data, split_ratio)<br/> print("Size of Training Set: ", len(training_set))<br/> print("Size of Testing Set: ", len(testing_set))<br/><br/> # create model<br/> summaries = summarize_by_label(training_set)<br/><br/> # test mode<br/> predictions = get_predictions(summaries, testing_set)<br/> accuracy = get_accuracy(testing_set, predictions)<br/> print('Accuracy: %'.format(accuracy))<br/> main('pima-indians-diabetes.data.csv', 0.70)</pre>
<h3 id="pros-of-the-algorithm">算法的优点</h3>
<ol>
<li>朴素贝叶斯算法是一种高度可扩展的快速算法。</li>
<li>二元和多元分类使用朴素贝叶斯算法。GaussianNB、MultinomialNB、BernoulliNB是不同种类的算法。</li>
<li>算法依赖于做一堆计数。</li>
<li>文本分类问题的绝佳选择。这是垃圾邮件分类的普遍选择。</li>
<li>它可以很容易地在小数据集上训练。</li>
</ol>
<h3 id="cons-of-the-algorithm">算法的缺点</h3>
<ul>
<li>根据“零条件概率问题”，如果给定的特征和类别的频率为0，则该类别的条件概率估计值为0。这个问题很麻烦，因为它也抹去了其他概率中的所有信息。"拉普拉斯校正。"是解决这个问题的示例校正技术之一。</li>
<li>另一个缺点是，它对独立性类特性做了强有力的假设。在现实生活中几乎不可能找到这样的数据集。</li>
</ul>
<h2 id="applications-of-naive-bayes-algorithm">朴素贝叶斯算法的应用</h2>
<p>朴素贝叶斯算法在多种现实场景中的应用有:</p>
<ol>
<li><strong>文本分类</strong>:用作文本分类的概率学习方法。当对文本文档进行分类时，即，无论文本文档属于一个还是多个类别，该算法都是最成功的算法。</li>
<li><strong>垃圾邮件过滤</strong>:文本分类的一个例子，是区分合法邮件和垃圾邮件的一种流行机制。许多现代电子邮件服务实施贝叶斯垃圾邮件过滤。一些服务器端的电子邮件过滤器，如SpamBayes、SpamAssassin、DSPAM、ASSP和Bogofilter，就利用了这种技术。</li>
<li><strong>情绪分析</strong>:用于分析推文、评论、评论的语气，即正面、中性、负面。</li>
<li><strong>推荐系统</strong>:朴素贝叶斯算法与协同过滤相结合，用于构建混合推荐系统，帮助预测用户是否喜欢某个给定的资源</li>
</ol>
<h2 id="conclusion">结论</h2>
<p>希望现在你已经理解了什么是朴素贝叶斯，文本分类利用了它。这个简单的方法对于分类问题非常有效，而且从计算上来说，它也非常便宜。无论用户是不是机器学习专家，他们都有工具来构建自己的朴素贝叶斯分类器。</p>
<p>你在哪里看到这个算法在使用？让我们知道！下面评论。</p>
<p><strong>人也在读:</strong></p>


									</div>

									</div>    
</body>
</html>