<html>
<head>
<title>Python Web Scraping Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Python网页抓取指南</h1>
<blockquote>原文：<a href="https://hackr.io/blog/python-web-scraping-guide#0001-01-01">https://hackr.io/blog/python-web-scraping-guide#0001-01-01</a></blockquote><div><div class="content">
										<p>网络抓取是一个过程，包括编写程序从网站上获取并解析公开可用的数据。根据网站的设计，这可能是对非结构化数据的简单提取，或者需要您模拟人类的行为，如点击链接或填写表格。</p>
<p>互联网是一个不断增长的网站集合，使其成为一个巨大的信息数据资源。通过利用web抓取技术，包括数据科学、商业智能等在内的一系列行业可以从这些信息中提取巨大的价值。</p>
<p>Python web抓取是完成这一活动最流行的方式之一。凭借直观的语法和一系列强大的第三方web抓取库，Python中的web抓取是从公共网站生成结构化数据的一种极佳方式。</p>
<h2 id="why-learn-python-web-scraping"><strong>为什么要学习Python网页抓取？</strong></h2>
<ul>
<li><strong>生成结构化数据:</strong>收集并转换一系列非结构化格式的公开网站数据</li>
<li><strong>自动化:</strong>取代缓慢繁琐的人工收集网页数据的过程，省时省力，提高生产力</li>
<li><strong>替代API: </strong>从不提供API或其他数据访问方式的网站中提取信息</li>
<li><strong>数据监测:</strong>追踪竞争对手、SEO、新闻开发、社交媒体等</li>
<li><strong>市场营销:</strong>市场分析、销售线索挖掘、市场趋势等</li>
</ul>
<h2 id="python-data-scraping-skills"><strong> Python数据抓取技巧</strong></h2>
<p>在开始使用Python进行数据采集之前，您需要理解各种概念。</p>

<ul>
<li><strong> Python基础:</strong>变量、数据类型、集合、循环、控制结构等</li>
<li><strong>领域对象模型(DOM): </strong>页面加载时浏览器创建的对象的树形结构，允许脚本访问/更新网页内容，结构&amp;风格</li>
<li><strong> HTML &amp; XML基础知识:</strong>构建和格式化网页，你需要理解标签和属性，Python才能成功抓取网站数据</li>
<li>HTTP方法:至少理解GET、POST、PUT和DELETE方法</li>
</ul>
<p>你是想开始网络抓取的Python初学者吗？要提高你的技能，请查看:</p>
<p><strong> <a class="btn btn-primary btn-call-to-action " href="https://hackr.io/blog/free-udemy-courses">最佳免费Udemy课程</a> </strong></p>
<h2 id="web-scraping-python-libraries"><strong>网络抓取Python库</strong></h2>
<p>幸运的是，我们可以从一系列<a href="https://hackr.io/blog/best-python-libraries">流行的Python库</a>中挑选来抓取web数据。我们选择了三个最受欢迎的第三方库，并在下表中比较了它们的主要特性。因此，如果你想知道如何用Python从网站上抓取数据，这可能会有所帮助。</p>
<p>虽然这些对于一般的网络抓取都是有用的，但是根据你的网络抓取目标和你的任务大小，知道什么时候和为什么使用这些工具是有帮助的。</p>
<table>
<tbody>
<tr>
<td> </td>
<td>
<p><strong>美汤</strong></p>
</td>
<td>
<p><strong>硒</strong></p>
</td>
<td>
<p><strong>刺儿头</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>易用性</strong></p>
</td>
<td>
<p>坦率的</p>
</td>
<td>
<p>中等</p>
</td>
<td>
<p>中等硬度</p>
</td>
</tr>
<tr>
<td>
<p><strong>速度</strong></p>
</td>
<td>
<p>快的</p>
</td>
<td>
<p>中等</p>
</td>
<td>
<p>快的</p>
</td>
</tr>
<tr>
<td>
<p><strong> JavaScript支持</strong></p>
</td>
<td>
<p>不</p>
</td>
<td>
<p>是</p>
</td>
<td>
<p>需要中间件</p>
</td>
</tr>
<tr>
<td>
<p><strong>学习曲线</strong></p>
</td>
<td>
<p>简单易学</p>
</td>
<td>
<p>中等</p>
</td>
<td>
<p>中等硬度</p>
</td>
</tr>
<tr>
<td>
<p><strong>文档</strong></p>
</td>
<td>
<p>优秀的</p>
</td>
<td>
<p>好的</p>
</td>
<td>
<p>好的</p>
</td>
</tr>
<tr>
<td>
<p><strong>优点</strong></p>
</td>
<td>
<p>简单分析器</p>
</td>
<td>
<p>广泛的浏览器支持</p>
</td>
<td>
<p>带有Spider的完整API</p>
</td>
</tr>
<tr>
<td>
<p><strong>缺点</strong></p>
</td>
<td>
<p>需要依赖关系</p>
</td>
<td>
<p>面向测试</p>
</td>
<td>
<p>缺少JavaScript支持</p>
</td>
</tr>
<tr>
<td>
<p><strong>用例</strong></p>
</td>
<td>
<p>从少量页面中提取HTML数据</p>
</td>
<td>
<p>JavaScript页面交互性，包括表单、导航等</p>
</td>
<td>
<p>需要高级功能的大型专业项目</p>
</td>
</tr>
</tbody>
</table>
<h2 id="web-scraping-tutorial-with-beautiful-soup"><strong>美汤网刮教程</strong></h2>
<p>在这个例子中，我们将通过Python库<em> BeautifulSoup4 </em>使用Python和Beautiful Soup进行web抓取。</p>
<p>我们将使用ArXiv，这是一个数学、物理、计算机科学、生物、金融等领域的科学论文的开放资源库。我们将通过获取标题、摘要和作者来关注人工智能论文。</p>

<p>在我们编写任何代码之前，我们需要访问网页，检查GUI和HTML内容。这样做表明论文以重复的格式列出，如下图所示。</p>
<p><img src="../Images/80059a397f5e6be2babf49c03fc35825.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678706y7CPP9IJJ2.png"/></p>
<p><em>从研究网页结构开始</em></p>
<p>然后，我们可以使用浏览器的开发工具来检查这个页面的DOM。您可以进入浏览器菜单，或者右键单击结果页面上的任何文章，然后选择<em> inspect，</em>，如下图所示。</p>
<p><img src="../Images/553f2e11bd1d9e7c130d87207b2ec5f5.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678717tza3s8HitR.png"/></p>
<p><em>使用开发者工具访问网页DOM</em></p>
<p>我们知道我们想要抓取与每篇论文相关的特定数据字段，所以我们需要检查DOM，直到找到包含这些数据的HTML元素。</p>
<p><img src="../Images/ce81cc0dd7b655135add72d65dc4c000.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678757PNvzOa2Ino.png"/></p>
<p><em>检查网页DOM以找到HTML元素</em></p>
<p>通过检查DOM中的HTML元素，我们可以看到论文包含在带有一个类<em>、【arxiv-result】、</em>的<em>、【李】、&gt;、</em>标签中。这些信息是必不可少的，我们将使用它在我们的Python程序中从网页的HTML内容中抓取论文。</p>
<p>我们研究的最后阶段是查看这些<em>、&lt;、李&gt;、</em>标签中的内部HTML元素，如下图所示。</p>
<p>我们可以看到，每个数据元素都包含在<em> &lt; p &gt; </em>标签中，用不同的类名来标识不同的数据字段。同样，这是我们的基本信息，我们将在Python代码中使用它来收集这些数据。</p>
<p><img src="../Images/09a2f1f1e63b6b0caf1ac952cd0110c4.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678771JREQNU8eOU.png"/></p>
<p><em>检查HTML元素以找到数据字段</em></p>
<p>我们现在准备用Python编写漂亮的Soup web抓取程序，以获取、处理和存储结构化数据文件中的数据。</p>
<p>查看下面的源代码，您会发现我们必须安装并导入<em>请求</em>和<em>美丽组4 </em>库。我们还导入了<em> json </em>和<em> csv </em>模块，以便在完成处理后保存结构化数据。</p>
<p><strong>源代码:</strong></p>
<pre class="language-python"><code>'''
Python Web Scraping: Beautiful Soup
-------------------------------------------------------------
pip install requests beautifulsoup4
'''


import requests
from bs4 import BeautifulSoup
import json
import csv


base_url = 'https://arxiv.org/search/?'
query = 'query=artificial+intelligence&amp;searchtype=all&amp;source=header'
request_url = base_url + query
response = requests.get(request_url)
soup = BeautifulSoup(response.text, 'html.parser')
papers = soup.find_all('li', class_='arxiv-result')

data = []
for paper in papers:
    title = paper.find('p', class_='title is-5 mathjax').text
    abstract = paper.find('p', class_='abstract mathjax').text
    authors = paper.find('p', class_='authors').text
    paper_data = {
        'Title': title,
        'Abstract': abstract,
        'Authors': authors
    }
    data.append(paper_data)

with open('papers.json', 'w') as f:
    json.dump(data, f)

with open('papers.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(["Title", "Abstract", "Authors"])
    for paper in data:
        writer.writerow(paper.values())</code></pre>
<p>我们从用<em>获取网页内容开始。从<em>请求</em>中获取()</em>方法。我们可以将它作为第一个参数传递给<em> BeautifulSoup </em>构造函数，第二个参数是一个<em>解析器</em>参数。我们选择了HTML解析器，因为我们的网页将是HTML内容。</p>
<p>通过创建一个<em> BeautifulSoup </em>对象，我们可以将页面的HTML内容表示为一个嵌套的数据结构对象，允许我们通过标签类型和标签属性进行搜索。</p>
<p>我们使用<em>。find_all() </em>方法返回嵌套结构中的所有HTML元素，嵌套结构是一个<em> &lt; li &gt; </em>标签，带有一个类<em>‘arxiv-result’</em>(我告诉过你这会很有用！).然后，我们可以用for循环遍历这个标签对象列表，允许我们处理每一篇文章。</p>
<p>通过迭代文章，我们可以使用<em>。find() </em>方法提取每个页面的标题、摘要和作者的数据字段。</p>
<p>我们使用在初步研究中发现的<em> &lt; p &gt; </em>标签类来确保获取正确的HTML元素。我们也使用<em>。text </em>属性来确保我们只返回包含在HTML标签中的文本。</p>
<p>每个循环都将关键信息添加到一个字典中，然后将它附加到我们的<em> data </em> list对象中，以将我们收集的数据保存到一个数据结构中。该阶段表示通过字典和列表将非结构化网页数据转换成结构化格式。</p>
<p>当我们浏览完文章后，我们使用两种方法将结构化数据保存到一个文件中。</p>
<p>首先，我们使用<em> json.dump() </em>将数据序列化为json格式，保存到一个JSON文件中。这对于与其他应用程序共享JSON数据非常有用。</p>
<p>其次，我们将数据保存到一个标准的CSV文件中，这样我们就可以在电子表格软件或其他应用程序中使用它。这要求我们循环遍历我们的<em>数据</em>列表中的每个元素，将每个元素作为一个新行写入CSV文件。</p>

<p>查看官方<a href="https://beautiful-soup-4.readthedocs.io/en/latest/" target="_blank" rel="noopener">文档</a>了解更多关于美汤的信息。</p>
<h2 id="python-scraping-tutorial-with-selenium"><strong>带硒的Python刮痧教程</strong></h2>
<p>在本例中，我们将使用Python库Selenium通过表单和按钮与网站进行交互。</p>
<p>我们将使用ChemNetBase网站，这是一个化学信息的综合数据库，包括化学工业中研究人员、学生和专业人员使用的结构、性质、光谱和反应。</p>
<p>我们将通过搜索名称或描述中包含“氨基”一词的化合物来保持相对简单。</p>
<p>在我们开始任何代码之前，我们需要对网站的HTML元素结构进行初步的研究。</p>
<p>首先，我们进入搜索页面检查GUI，如下图所示。这让我们能够识别将与Python程序交互的搜索页面元素。</p>
<p><img src="../Images/2b46644a25611fe17b23cd7b36559805.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678789XEfwmNgL70.png"/></p>
<p><em>研究搜索页面的GUI结构</em></p>
<p>我们可以通过右键单击搜索栏和搜索按钮来检查搜索页面的DOM，然后选择<em>检查</em>。这突出了我们需要与之交互的DOM元素，如下图所示。</p>
<p><img src="../Images/ab1debd69e49acdbb880a006e22b427d.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678804UOLpleAkx4.png"/></p>

<p><em>检查搜索页面DOM以找到输入的&amp;按钮元素</em></p>

<p>我们可以看到，搜索栏包含在id为<em> 'searchForm:searchTerm1' </em>的<em> &lt; input &gt; </em>元素中，搜索按钮包含在id为<em> 'searchForm:j_idt101' </em>的<em> &lt;按钮&gt; </em>元素中。这对我们的Python程序至关重要。</p>
<p>第二步是检查搜索结果页面，找到包含我们想要抓取的数据的HTML元素。我们将运行对术语“amino*”的搜索，然后右键单击任何结果来<em>检查</em>DOM，如下图所示。</p>
<p><img src="../Images/2964b5c8ab03a783892acf37c8fcd6c4.png" alt="" data-original-src="https://cdn.hackr.io/uploads/posts/attachments/1673678817MCG64IzqTy.png"/></p>
<p><em>检查结果页面DOM以找到HTML元素</em></p>
<p>我们看到结果包含在一个<em> &lt;表&gt; </em>中，该表带有一个<em> &lt; tbody &gt; </em>元素，该元素的id为<em>‘product tabs:results form 0:hitrowsTbl _ data’</em>。这代表了完整的结果集，但是我们需要通过访问单个result <em> &lt; tr &gt; </em>元素来用Python抓取数据。</p>
<p>我们现在准备编写Python程序来与网站交互，获取数据，处理数据，并将其存储在结构化的CSV文件中。</p>
<p>查看下面的源代码，您会发现我们必须安装Selenium并从Selenium库中导入各种类。</p>
<p>其核心是Selenium <em> WebDriver </em>，这是一个简单但功能强大的面向对象的API，允许我们在浏览器中以编程方式与网页进行交互。</p>
<p>我们将使用谷歌Chrome，这意味着我们需要从<em> selenium.webdriver </em>导入<em> Chrome </em>。</p>
<p>我们还将从<em>selenium . web driver . chrome . Options</em>模块中导入<em>选项</em>类。通过创建这样的一个实例，我们可以将我们的<em> WebDriver </em>对象设置为<em> headless </em>，允许我们的Chrome浏览器在后台运行而无需加载GUI。</p>

<p>根据您的操作系统，您可能需要从源代码中的链接下载并安装ChromeDriver。Selenium在与网页交互时用这个来控制你的Chrome浏览器。</p>

<p>如果你更喜欢使用Firefox、Safari或其他浏览器，我们的源代码提供了一个你可以修改的模板。我们建议前往官方<a href="https://www.selenium.dev/documentation/" target="_blank" rel="noopener">文档</a>获取每个浏览器的具体细节。</p>
<p>您还会看到，我们从<em>selenium . web driver . common . By</em>模块中导入了<em> By </em>类来帮助定位网页元素，并从<em>selenium . common . exceptions</em>模块中导入了<em> TimeoutException </em>来处理网页超时。</p>
<p><strong>源代码:</strong></p>
<pre class="language-python"><code>'''
Python Web Scraping: Selenium
-------------------------------------------------------------
Follow instructions to Download chromedriver from:
https://chromedriver.chromium.org/downloads/version-selection

pip install selenium
'''


from selenium.webdriver import Chrome
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
import csv

opts = Options()
opts.headless = True
timeout = 10
url = 'https://www.chemnetbase.com'

with Chrome(options=opts) as driver:
   driver.get(url)
   driver.implicitly_wait(timeout)
   try:
       search_form = driver.find_element(By.ID, 'searchForm:searchTerm1')
       print('Page is ready!')
       search_form.send_keys('amino*')
       search_button = driver.find_element(By.ID, 'searchForm:j_idt101')
       search_button.click()

   except TimeoutException:
       print('Search page loading took too much time!')

   try:
       result_table = driver.find_element(
           By.ID, 'PRODUCTtabs:resultsForm0:hitrowsTbl_data')
       print('Results are ready!')
       result_rows = result_table.find_elements(By.TAG_NAME, 'tr')
       data = []
       for row in result_rows:
           cols = row.find_elements(By.TAG_NAME, 'td')
           name = cols[1].text
           synonyms = cols[2].text
           molec_form = cols[3].text
           CAS_num = cols[5].text
           data.append([name, synonyms, molec_form, CAS_num])

       with open('amino.csv', 'w') as f:
           csv_writer = csv.writer(f)
           csv_writer.writerow(
               ['Name', 'Synonyms', 'Molecular Formula', 'CAS Number'])
           csv_writer.writerows(data)

   except TimeoutException:
       print('Results loading took too much time!')​</code></pre>
<p>现在我们已经导入了所有必要的模块和类，我们可以开始破解了！</p>
<p>我们首先创建一个<em>选项</em>对象，并将headless属性设置为True。我们还必须确保<em> WebDriver </em>允许通过使用带有超时参数的隐式等待来加载页面元素。</p>
<p>这告诉<em> WebDriver </em> to <em> poll </em>在抛出<em> TimeOutException </em>之前在定义的时间段内查询页面的DOM。在我们的例子中，我们设置了一个10秒的超时变量来使用<em>。来自<em> WebDriver </em>类的implicitly_wait() </em>方法。</p>
<p>代码主体使用Python上下文管理器来创建(并自动退出)我们的<em> WebDriver </em>对象。然后我们可以称之为<em>。让<em> WebDriver </em>类的get() </em>方法加载我们的URL，后面是上面概述的隐式等待方法。</p>
<p>我们为搜索页面使用了一个try-except块来捕捉任何超时。如果<em> WebDriver </em>在10秒内没有找到我们的HTML元素，它将抛出一个<em>超时异常</em>。</p>
<p>我们已经使用了<em>。来自<em> WebDriver </em>类的find_element() </em>方法来定位我们的搜索栏。这就用到了<em>。ID </em>属性从<em> By </em>类传入搜索栏的HTML元素ID(来自我们的初步研究)。</p>
<p>在搜索栏加载之后，我们可以调用<em>。send_keys() </em>方法将' amino* '输入到无头浏览器的搜索栏中。在调用<em>之前，我们可以用同样的方法找到搜索按钮。单击()</em>完成我们的搜索并加载结果页面。</p>
<p>在这个阶段，我们已经结束了我们的网页互动，并准备刮的结果。</p>
<p>我们再次使用try-except块来处理结果页面的超时。加载结果后，我们使用<em>。来自<em> WebDriver </em>类的find_element() </em>方法来定位带有我们在研究DOM时发现的ID的结果表。</p>
<p>然后我们称之为<em>。find_elements() </em>方法对table元素返回一个<em>列表中</em>的单个<em>&lt;tr&gt;T5】元素。这就需要我们使用<em>。Tag_Name </em>属性来自<em> By </em>类。</em></p>
<p>在这个阶段，我们可以迭代我们的<em> &lt; tr &gt; </em>元素列表，允许我们调用<em>。在每个上找到_elements() </em>并提取一个列表中的<em>&lt;&gt;</em>元素。这些表示我们想要抓取的数据字段。</p>
<p>接下来就是从<em> &lt; td &gt; </em>元素列表中索引我们想要的值，并将这些值赋给变量。注意，我们使用了<em>。text </em>属性返回标签内的文本。</p>
<p>然后，我们可以将变量添加到存放我们结果的列表<em>中，为我们收集的数据提供结构化的数据格式。</em></p>
<p>最后一步是将我们的数据保存到标准的CSV文件中。我们通过遍历列表中的每个元素并将每个元素作为新的一行写入CSV文件来实现这一点。</p>
<p>如果您想了解更多关于使用Selenium的知识，请查看官方的<a href="https://www.selenium.dev/documentation/" target="_blank" rel="noopener">文档</a>。</p>
<h2 id="best-practices-for-python-web-scraping"><strong>Python网页抓取的最佳实践</strong></h2>
<p>遵循最佳实践以确保您的网络抓取活动合法、道德、高效，并且不会对您想要抓取的网站造成损害，这一点很重要。</p>
<ul>
<li>遵守服务条款&amp; robots.txt :遵守网站的服务条款和robots.txt文件中的限制，负责任地、合乎道德地进行刮擦</li>
<li><strong>代理&amp;轮换IP地址:</strong>网站可能会阻止提出过多请求的IP地址，因此使用代理或轮换您的IP地址以避免被阻止</li>
<li><strong>处理验证码:</strong>验证码(完全自动化的公共图灵测试来区分计算机和人类)会阻止自动抓取，因此您可能需要手动解决这些问题或使用验证码解决服务</li>
<li><strong>避免过多的请求</strong>:如果你在短时间内发出大量的请求，抓取会使网站的服务器不堪重负，所以要注意你的负载以避免拒绝服务</li>
<li><strong>适当的用户代理:</strong>在HTTP报头的用户代理字段中，将自己标识为web scraper，以帮助网站所有者在Python scraper出现问题时阻止它</li>
<li><strong>导出数据:</strong>抓取数据后，您需要通过写入数据库、文件(txt、CSV、JSON等)或编程数据结构来存储数据，以供将来分析</li>
</ul>
<h2 id="common-data-scraping-challenges"><strong>常见数据收集挑战</strong></h2>
<p>无论您有多少经验，任何使用web抓取从互联网上收集非结构化数据的人都会面临一些共同的挑战。</p>
<ul>
<li><strong>网站变化:</strong>网站不断更新，这可能会阻止您的自动网络抓取程序提取数据，直到您更改您的抓取代码</li>
</ul>
<ul>
<li><strong>屏蔽或限速:</strong>许多网站试图通过验证码测试、基于IP的限速或服务屏蔽来阻止或限制网络抓取</li>
</ul>
<ul>
<li><strong>认证:</strong>一些网站需要登录才能访问内容或数据，这要求您的Python web scraper在抓取数据之前与网页元素进行交互</li>
</ul>
<ul>
<li><strong>质量差的数据:</strong>如果网络抓取程序收集不准确、过时或不可靠的数据，会影响对数据进行的任何分析的质量</li>
</ul>
<ul>
<li><strong>法律问题:</strong>网络抓取可能会引起争议，被一些人视为一种未经授权的网站访问，导致服务条款禁止网络抓取</li>
</ul>
<ul>
<li><strong>道德考量:</strong>如果网络抓取程序在用户不知情或未同意的情况下收集个人数据，可能会引发道德问题</li>
</ul>
<h2 id="web-scraping-alternatives"><strong>网页抓取替代方案</strong></h2>
<p>Web抓取是一种自动化和加速从网页收集原始数据过程的极好方法，但是这不是从互联网收集数据的唯一方法。</p>
<ul>
<li><strong>使用API:</strong>越来越多的网站提供API供你从他们的平台上访问和检索数据。这可能是一种更可靠、更高效的数据访问方式，无需直接抓取网站。</li>
</ul>
<ul>
<li><strong> Web数据集成:</strong>一些网站提供预打包的数据馈送，可以轻松集成到您的应用程序或平台中。这可能是从网站访问数据的更直接的方式，因为它避免了编写自定义抓取脚本的需要。</li>
</ul>
<ul>
<li><strong>手动数据输入:</strong>在某些情况下，将数据从网站手动输入到您的应用程序或平台可能会更高效。虽然很费时间，但如果您需要的数据无法通过其他方式轻易获得，这可能是必要的。</li>
</ul>
<h2 id="conclusion"><strong>结论</strong></h2>
<p>随着包含有价值且通常是非结构化数据的网站集合的不断增长，对于数据科学和商业智能等行业而言，互联网是一个巨大的信息数据资源。使用网络搜集技术，我们可以编写程序从这些网站获取并解析公开可用的数据。</p>
<p>Python web scraping是完成这一活动最流行的方法之一，因为它具有直观的语法和一系列第三方web scraping库。</p>
<p>本文介绍了如何使用Python进行webscrape的基础知识，包括web scrape最流行的第三方库的比较、最佳实践和常见挑战。</p>
<p>我们还为一个简单的HTML网站介绍了一个带有漂亮的Soup库的Python数据刮刀的详细示例。这允许我们抓取数据、处理数据，并将其保存到结构化的JSON和CSV文件中。</p>
<p>然后，我们用Selenium研究了一个更复杂的Python web抓取示例。这需要我们使用Selenium <em> WebDriver </em>来控制一个<em>无头</em>(无GUI)浏览器，从而与网页元素进行交互。我们进行了搜索，抓取了搜索结果，然后在将它们保存到结构化的CSV文件之前对它们进行了处理。</p>
<p><strong>正在寻找增强Python技能的方法吗？接下来读这个:</strong></p>
<p><strong> <a class="btn btn-primary btn-call-to-action " href="https://hackr.io/blog/python-frameworks">十大最佳Python框架</a> </strong></p>
<h2 id="frequently-asked-questions"><strong>常见问题解答</strong></h2>
<h4 id="toc-1-is-python-good-for-web-scraping"><strong> 1。Python好不好做网页抓取？</strong></h4>
<p>Python是web抓取的流行选择，因为它简单易学，提供了一系列第三方web抓取库和HTML解析工具，并为web抓取活动提供了优秀的文档和社区支持。</p>
<h4 id="toc-2-which-module-is-used-for-web-scraping-in-python"><strong> 2。Python中网页抓取用的是哪个模块？</strong></h4>
<p>Python为web抓取提供了几个模块，包括requests、Beautiful Soup、Selenium、Scrapy等等。用于Python web抓取的最佳模块取决于您的web抓取项目的范围、您是否需要与JavaScript元素交互，以及您自己的Python编程经验。</p>
<h4 id="toc-3-how-long-does-it-take-to-learn-python-web-scraping"><strong> 3。学习Python网页抓取需要多长时间？</strong></h4>
<p>这取决于您以前的Python编程知识以及您对HTML、XML、DOM和HTTP的一般理解。</p>
<p>如果你已经熟悉了这些技巧，你可以在几个小时内开始刮，尽管如果你是一个绝对的初学者，可能需要更长的时间。不管怎样，我们的例子应该能让你快速掌握基本技能。像所有的编程技巧一样，精通需要时间，你在网页抓取上投入的时间越多，你就会做得越好。</p>
<h4 id="toc-4-is-it-legal-to-scrape-a-website"><strong> 4。刮网站合法吗？</strong></h4>
<p>一般来说，网页抓取并不违法，但根据网站的服务条款、数据保护法和版权法，会有一些限制。有必要了解这些法律问题，以确保您的网络抓取活动在法律范围内。</p>
<h4 id="toc-5-what-is-the-best-web-scraping-language"><strong> 5。最好的网页抓取语言是什么？</strong></h4>
<p>有几种编程语言可以用于web抓取，包括Python、Java和Ruby。Python因其库和框架而受欢迎，而Java因稳定性和性能而闻名。Ruby也是一个不错的选择，因为它的简单性和丰富的库。</p>
<p>特定项目的最佳语言取决于您的技能和项目需求。</p>

									</div>

									</div>    
</body>
</html>