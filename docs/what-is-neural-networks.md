# 什么是神经网络？[定义] -初学者指南

> 原文：<https://hackr.io/blog/what-is-neural-networks>

## **神经网络简介**

神经网络或也称为人工神经网络(ANN)是利用复杂数学模型进行信息处理的网络。它们基于人类大脑中神经元和突触的功能模型。与人脑类似，神经网络连接简单的节点，也称为神经元或单元。这些节点的集合形成了一个节点网络，因此被称为“神经网络”

与人脑类似，在神经网络中，一系列算法用于识别和识别数据集中的关系。神经网络被设计成适应动态输入场景；结果，网络提供了最好的可能结果，而不必为了进一步处理而重新设计输出。

从应用的角度来看，神经网络正被用于各种技术和应用，例如视频游戏、计算机视觉、语音识别、社交网络过滤、棋盘游戏、机器翻译和医疗诊断。令人惊讶的是，神经网络正被用于传统和创造性的活动，如绘画和艺术。

## **神经网络的组成**

在这一点上，知道和理解什么构成了神经网络及其组件是很重要的。

### **1。神经元**

神经网络由类似于神经元生物模型的人工神经元组成。它接收数据输入，然后将输入与其内部激活状态以及可选的阈值激活功能相结合。然后通过使用输出函数，它产生输出。

初始输入是来自各种外部来源的数据，例如语音文件、图像和文档。最终的输出可以是识别语音输入或图像或文本中的对象。激活函数的重要性在于，当输入值动态变化时，它提供了一个无缝且可区分的过渡。因此，输入数据的微小变化或偏移会导致输出的微小变化。

### **2。连接和重量**

神经网络由连接和权重组成，其中每个连接抛出一个神经元的输出，该输出成为网络中另一个神经元的输入。为每个连接分配一个权重，它表示它在神经网络上的相对重要性。任何给定的神经元都可以与多个输入和输出连接有多对多的关系。

### **3。组织**

它是将神经元组织成多层。这恰好适用于深度学习领域。它的设计方式是神经元与紧邻的神经元层相连。这意味着一层的神经元只连接到紧邻的前一层和紧邻的后一层的神经元。输入层是接收外部数据的层，交付最终结果的层是输出层。它们之间可以有更多的隐藏层或者没有层。在某些情况下，非分层和单层网络也是可能的，并且两层之间可能有多种连接模式。它是如此的通用和最大，以至于一个完全连接的神经元集是可能的，其中一层中的每个神经元都连接到下一层中的每个神经元。它非常灵活，可以将一组神经元集中在一层，连接到相邻层的单个神经元，该层的神经元数量减少。这种连接产生了所谓的前馈网络，只有这样的连接才形成有向无环图。此外，允许在前一层或相同层中的神经元之间连接的网络被称为递归网络。我们将在本文后面看到神经网络的类型。

### **4。超参数**

超参数是其值为常数的初始参数，并且在神经元的学习过程开始之前设置。参数的后续值是在学习过程中导出的。超参数的一些例子是学习率、隐藏层数和批量大小。一些超参数的值可以依赖于其他超参数的值。例如，总层数可能依赖于某些层的大小。

### **5。学习**

学习是网络通过将样本数据观察值考虑在内来调整自身以更好地处理任务的过程。学习包括校准网络的权重和可选阈值，以获得更准确的结果。这是通过最小化观察到的误差来执行和实现的。当被检查的额外观察值无助于降低错误率时，学习过程达到最佳状态。必须注意的是，即使在学习过程完成后，大多数场景下的错误率也不会达到“0”。如果即使在学习过程之后错误率仍然太高，则需要重新设计网络。

在实际应用中，定义了一个代价函数，并在学习过程中定期对其进行评估。只要产出数字继续下降，学习过程就会继续。成本函数经常被评估，并且被定义为只能被近似的统计值。一个例子是对单词“狗”的学习过程。输出是数字，当观察到的误差较低时，输出值(几乎肯定是一只狗)和正确答案(狗)之间的差异非常小。学习过程试图减少观察值的总体差异。大多数学习模型使用优化理论和统计估计。

### **6。学习率**

每个观察值的学习率定义了模型用来调整误差的校正步骤的大小。高学习率可以减少训练时间，但是输出可能不太准确。因此，在实践中，较低的学习率是首选，这需要较长的时间，但有可能提供更高的准确性。诸如 Quickprop 等优化技术的主要目标是提高误差最小化的速度。其他学习改进技术主要试图实现分数的更高可靠性。

为了提高误差最终减小到最大值时的收敛速度，并避免网络内部的振荡，例如交替连接权重，使用根据需要增加或减少的自适应学习速度模型。有一个“动量”的概念，它允许对梯度和先前变化之间的平衡进行加权，以便重量校准在一定程度上依赖于先前的变化。接近“0”的动量表示梯度，而接近“1”的值表示最后的变化。

### **7。成本函数**

简单来说，成本函数衡量神经网络模型的性能。它还可能依赖于其他变量，如权重和偏差。成本函数不是向量，而是单个值。它提供了一种衡量神经网络整体表现的方法。定义一个特别的成本函数是可能的。然而，在大多数情况下，成本函数的值的选择是由函数的期望属性(如凸性)导出的，或者是因为成本函数的输入来自于模型，如后验概率可用作逆成本的概率模型。

### **8。传播函数**

传播函数根据前一个神经元及其连接的输出来计算该神经元的输入的加权和。可以将偏差项附加到传播函数的结果上

### **9。反向传播**

反向传播是一种用于调整连接权重的方法，以校准学习过程中的每次错误发生。误差值以相等的方式在连接中分配。在技术术语中，在反向传播过程中，梯度即计算与权重的给定状态相关联的成本函数的导数。通常，使用随机梯度下降建模或其他方法来完成对权重的更新。

[深度学习 A-Z:动手人工神经网络](https://click.linksynergy.com/deeplink?id=jU79Zysihs4&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdeeplearning%2F)

## **神经网络的类型**

### **1。前馈神经网络**

这是最简单的神经网络模型。前馈神经网络在使用时速度很快；但是从训练的角度来说，有点慢，需要时间。大多数视觉和语音识别应用使用某种形式的前馈型神经网络。

前馈网络是非线性的。这些网络被称为前馈网络的主要原因是，数据流更多地发生在正向，因此数据以单向方式传输，即。输入到输出。可以安排不同的函数来描述这些网络。每个模型都可以描述为一个图表，其中描述了功能组。例如，三个函数 f(1)，输入层 1，f(2)是层 2，f(3)是输出层。因此，信息从输入层传递到进行计算的下一层，再传递到输出层。

### **2。径向基函数(RBF)神经网络**

在这种类型的神经网络中，数据根据其与中心点的距离进行分组。在没有训练数据的情况下，数据被分组，并创建中心点。这个网络被设计成寻找彼此相似的数据点，然后将数据分组。这种类型的神经网络的一个示例应用是 [**电力恢复系统**](https://link.springer.com/article/10.1007/s40565-016-0219-2) 。

为了更好地理解，进一步解释，径向基函数(RBF)神经网络具有三层-输入层、隐藏层和输出层。隐藏层是非线性的，而输出层是线性的。RBF 网络的应用是图像处理、语音识别和医疗诊断。

#### **RBF 网络-三层-详情:**

**1。输入层**

对于每个预测变量，在输入层有一个神经元，在分类变量的情况下，使用 N-1 个神经元，其中 N 表示类别的数量。数值范围的标准化由输入神经元执行，其中减去中间值并除以四分位数范围。随后，输入神经元将每个值提供给隐藏层中的神经元。

**2。隐藏层**

这一层由数量可变的神经元组成，训练过程决定了确切的数量。每个神经元包含一个以一个点为中心的径向基函数。每个神经元的维数和预测变量的数目是相同的。对于每个维度，RBF 函数的扩散或半径可以不同。训练过程定义并确定中心和分布。隐藏神经元计算测试用例到神经元中心点的欧几里德距离。在使用扩展值将 RBF 核函数应用于距离之后，由此获得的值被传递到求和层。

**3。求和层**

来自隐藏层的神经元的输出值被乘以与该神经元相关联的权重，并被移位到求和函数，在该求和函数中，加权值被相加，并且该和被表示为网络的输出。每当出现分类难题时，都会有一个输出，其中包含一组单独的权重和一个针对每个类别目标的求和单元。输出值是正在研究或评估的案例具有该特定类别的概率。

RBF 网络与 K 均值聚类、PNN 和 GRNN 网络完全相同。

**关键事实**

*   在 PNN/GRNN 网络中，训练文件中的每个点都有一个神经元。在 RBF 网络的情况下，神经元的数量是可变的，通常少于训练点的数量。
*   在中小型训练集中，PNN 或 GRNN 网络通常比 RBF 网络更准确。缺点是 PNN 或 GRNN 网络实际上不适合大的训练集。

### **3。Kohonen 自组织神经网络**

由 [Brain Corporation](http://www.braincorp.com/) 支持的 per-scholar pedia 表示，“Kohonen 网络，也称为自组织映射(SOM)，用于可视化和分析高维数据，特别是通过实验获得的信息。这是一种计算方法，它定义了一个有序的映射，并从一组给定的数据点投影到一个规则的二维网格上。

SOM 主要用于度量向量分布的可视化，比如度量值和统计属性的有序集合。实际上可以表明，数据的相互成对距离可以通过利用任何数据集或项目的 SOM 类型映射来定义。SOM 计算方法也可以应用于非矢量数据集，例如符号串和有机分子中的片段序列。"

### **4。递归神经网络**

在递归神经网络(RNN)中，前一步的输出作为下一步的输入。在传统的神经网络中，整个输入和输出是相互独立的。然而，当需要预测句子的下一个单词时，就需要前面的单词，这就需要记住前面的单词。因此，RNN 被开发出来，并被设计用来解决这个问题，即在隐藏层的帮助下记住以前的输入。

RNN 最重要的特征是它的隐藏状态，在那里关于一个序列的信息被记忆。它有“记忆”，记忆和回忆在前面的计算步骤中已经计算出的所有信息。在 RNN 中，对每个输入使用相同的参数，因为无论任务是对输入还是对隐藏层执行计算来产生输出，任务都是相同的。与其他神经网络相比，这极大地降低了参数的复杂性。RNN 有各种各样的应用，其中之一是 TTS(文本到语音)合成。

**递归神经网络的优势**

*   RNN 足够聪明，能够记住网络上的每一条信息，在时间序列预测中非常有用。这是它在这类应用中使用的主要原因，因为它也能记住以前的输入。
*   在这个模型中可以处理任何长度的输入。

**递归神经网络的缺点**

*   爆炸和渐变消失在这个模型中很常见。
*   训练 RNN 是一项相当具有挑战性的任务。
*   如果使用“tanh”或“relu”作为激活功能，它不能处理很长的序列。

### **5。卷积神经网络**

机器学习，更具体地说，深度学习的一个众所周知的算法是一个[卷积神经网络](https://hackr.io/blog/convolutional-neural-network) (CNN 或 ConvNet)。在 CNN 中，模型学习直接从图像、视频、文本或声音中执行任务。CNN 在图像和图片中寻找模式来识别物体、面孔和场景。学习直接从图像数据中发生。它们通过使用模式对图像进行分类，并且消除了对特征提取的人类交互的需求。

有趣的是，CNN 在需要物体识别和计算机视觉的应用方面非常强大，比如自动驾驶汽车和人脸识别。基于该应用，可以从头开始开发 CNN。预先训练的模型也可以用于数据集。CNN 在图像识别和模式检测方面设计得非常好。GPU 和并行计算的进步使得 CNN 非常强大，能够在自动驾驶和面部识别方面提供高质量的服务。CNN 是通用的，因为它们学会识别交通信号和行人之间的差异。

**CNN 为什么有用？**

*   CNN 消除了人工或人为干预特征提取工作的需要。
*   CNN 在识别结果中提供最高质量的结果。
*   CNN 使得能够建立在预先存在的网络上，从而使新的识别任务的再训练成为可能。
*   输入特征的微观管理是可能的。从这个意义上说，输入要素是作为批处理来处理的。这使得网络可以分几部分记忆一幅图像。

### **6。模块化神经网络(MNN)**

在模块化神经网络中，结果由几个独立的网络共同贡献。这些独立的神经网络执行由这些神经网络中的每一个构建的几个子任务。与其他神经网络相比，这种类型的活动提供了一组独特的输入。

此外，在这种类型的神经网络中，模块化有助于降低待解决问题的复杂性，因为这些模块化网络将计算过程完全分解成小的组成部分。由于连接的数量被分解，计算的速度也显著提高，因此降低了这些神经网络之间交互的要求。此外，处理的总时间取决于计算过程中涉及的神经元的数量。

一个有趣的事实是，模块化神经网络可能是[人工智能](https://hackr.io/blog/what-is-artificial-intelligence)中发展最快的领域。

## **结论**

正如我们已经看到的，有几种类型的神经网络，其中每一种都适用于不同的要求，以实现预期的结果。神经网络的重要之处在于，它们是按照大脑中神经元的工作方式进行建模设计的。因此，最终可以预期的是，随着更多的数据和利用率，这些网络将学习更多的知识并提高更多。传统的机器学习(ML)算法和神经网络的区别在于，ML 在一个点之后会趋于停滞。相比之下，随着数据和使用的增加，神经网络的性能和结果确实会增长。

这就是为什么许多行业专家坚定地认为神经网络将是下一代人工智能建立和发展的基本框架的原因之一。当然，到现在为止，您应该已经很好地理解了神经网络的概念及其类型。

**人也在读:**