<html>
<head>
<title>Understanding K-means Clustering in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>理解机器学习中的K-均值聚类</h1>
<blockquote>原文：<a href="https://hackr.io/blog/k-means-clustering#0001-01-01">https://hackr.io/blog/k-means-clustering#0001-01-01</a></blockquote><div><div class="content">
										<p>在深入研究算法之前，让我们先了解一些算法的背景知识。K-means聚类是一种机器学习 <span>算法。准确地说，机器学习算法大致分为有监督的和无监督的。无监督学习被进一步分类为数据集的变换和聚类。聚类还有几种类型，K-均值属于层次聚类。</span></p>
<p>在开始详细研究算法之前，让我们对这些概念有一个概述。</p>
<h2 id="what-is-unsupervised-learning"><strong>什么是无监督学习？</strong></h2>
<p>在没有任何指导的情况下，机器对未标记的数据进行训练，它应该发现数据中隐藏的模式。无监督学习算法执行复杂的任务，但与自然学习方法相比可能更令人怀疑。无监督方法允许找到对分类有用的特征。此外，所有未知模式都可以使用无监督学习找到。无监督学习的问题分为聚类和关联问题。</p>
<p><span>要详细了解无监督学习的更多信息<a href="https://hackr.io/blog/what-is-unsupervised-learning">访问这里</a>和<a href="https://hackr.io/blog/supervised-vs-unsupervised-learning">查看这里</a>无监督学习和有监督学习的区别。</span></p>

<p><span>现在让我们看看什么是集群:</span></p>
<h2 id="what-is-clustering"><strong>什么是聚类？</strong><strong/><strong/><strong/><strong/></h2>
<p><span>让我们考虑一个由点组成的数据集:</span></p>
<p><span><img src="../Images/17fef30820fe39f8203d314881ceec19.png" alt="dataset of points" data-original-src="https://hackr.io/blog/media/dataset-of-points.png"/>T2】</span></p>
<p><span>我们假设有可能找到一个标准(不是唯一的)，这样每个样本都可以与一个特定的组相关联:</span></p>
<p><span><img src="../Images/fb4f574241e6d4108d69bd1d57046dea.png" alt="clustering specific group" data-original-src="https://hackr.io/blog/media/clustering-specific-group.png"/>T2】</span></p>
<p><span/><span/><span/><span/><span/></p>

<p>按照惯例，每个组被称为一个聚类，并且寻找函数G的过程被称为聚类。聚类被认为是在一堆未知数据中发现结构或模式的一个重要概念。聚类算法处理数据并发现数据中存在的自然聚类(组)。这取决于用户来调整算法应该识别的聚类的数量，因为算法给出了修改组的粒度的能力。</p>
<p><span>您可以使用各种类型的聚类:</span></p>
<ul>
<li><span> <strong>分区:</strong>数据是这样组织的，单个数据只能是一个集群的一部分。</span></li>
<li><span> <strong>凝聚:</strong>每一个数据在这种技术中都是一个聚类。</span></li>
<li><span> <strong>重叠:</strong>在这种技术中，使用模糊集对数据进行聚类。</span></li>
<li><span> <strong>概率性:</strong>这种技术中使用概率分布来进行聚类。</span></li>
<li><span> <strong>层次化:</strong>这种算法使聚类具有层次性。它从分配给自己的群集的所有数据开始。那么两个聚类将在同一个聚类中。当只剩下一个聚类时，算法结束。</span></li>
<li><span> <strong> K均值聚类:</strong> K指的是枯燥的聚类算法，有助于找到每个问题的最高值。在这种聚类方法中，选择所需数量的聚类，将数据点聚类成k个组。更大的k-意味着更小的组具有更大的粒度，而更小的k意味着更大的组具有更小的粒度。</span></li>
</ul>
<p><span>现在让我们详细研究k-means聚类算法:</span></p>
<h2 id="k-means-clustering-algorithm"><strong> K均值聚类算法</strong></h2>
<p><span>k-means算法基于初始条件，通过分配k个初始质心或均值来决定聚类数:</span></p>
<p><span><img src="../Images/6ed19ae26a82686e8b8470ed17757ea1.png" alt="k initial centroids or means" data-original-src="https://hackr.io/blog/media/k-initial-centroids-or-means.png"/>T2】</span></p>
<p>然后计算每个样本和每个质心之间的距离，并将该样本分配给距离最小的聚类。这种方法通常被称为最小化集群的惯性，其定义如下:</p>
<p><span> </span></p>
<p><img src="../Images/35cd138eb7d3b08f904a47847aaec33c.png" alt="Eq 10" data-original-src="https://hackr.io/blog/media/eq-10.png"/></p>
<p><span> </span></p>
<p>该过程是迭代的，一旦所有的样本被处理，一组新的质心K被计算，并且所有的距离被重新计算。当达到所需容差时，或者换句话说，当质心变得稳定时，算法停止，因此惯性最小。</p>
<h2 id="algorithmic-steps"><strong>算法步骤</strong></h2>
<p><span>设X = { x </span> <span> 1 </span> <span>，x </span> <span> 2 </span> <span>，x </span> <span> 3 </span> <span>，……，x </span> <span> n </span> <span> }为数据点an </span>的集合</p>
<p><span> μ = {μ </span> <span> 1 </span> <span>，μ </span> <span> 2 </span> <span>，μ </span> <span> 3 </span> <span>，........，μ </span> <span> n </span> <span> }是中心点。</span></p>
<ol>
<li>随机选择“C”个聚类中心。</li>
<li><span>计算每个数据点和聚类中心之间的距离。</span></li>
<li><span>离聚类中心距离最小的数据点被分配给聚类中心。</span></li>
<li><span>用公式矩形化新的聚类中心:</span></li>
</ol>
<p><img src="../Images/abcae8c8c6483baa182c98a3d9a776a5.png" alt="Eq 2" data-original-src="https://hackr.io/blog/media/eq-2.png"/></p>
<p><span> </span> <span>其中ci表示第I个聚类中数据点的数量</span></p>
<ol>
<li><span>重新计算距离b/w新获得的聚类中心和数据点。</span></li>
<li><span>如果没有重新分配数据点，则停止，否则从步骤3开始重复。</span></li>
</ol>
<h2 id="sample-data-set-explaining-k-means-clustering"><strong>样本数据集讲解K-均值聚类</strong><strong/><strong/><strong/><strong/></h2>
<p><span>考虑一个带有虚拟数据集的简单示例:</span></p>
<p><span> </span> <span> </span> <span> </span> <span> </span></p>
<pre>from sklearn.datasets import make_blobs<br/>nb_samples = 1000<br/>X, _ = make_blobs(n_samples=nb_samples, n_features=2, centers=3, cluster_std=1.5 </pre>
<p><strong> </strong> <strong> </strong> <strong> </strong> <strong> </strong></p>
<p>在我们的例子中，由于每个斑点的标准偏差，我们有三个具有二维特征和部分重叠的聚类。我们在这里没有使用变量，因为我们想生成一组局部一致的点来测试我们的算法:</p>
<p><span><img src="../Images/6b41bd6a239376dcddca0656560fbaee.png" alt="coherent points 1" data-original-src="https://hackr.io/blog/media/coherent-points-1.png"/>T2】</span></p>
<p><span>在这种情况下，我们期望k-means在有界于[-5，0]之间的X-区域中以最小误差分离三个组。因此，保留我们得到的默认值:</span></p>
<p><span/><span/><span/><span/><span/></p>
<pre>from sklearn.cluster import KMeans<br/>&gt;&gt;&gt; km = KMeans(n_clusters=3)<br/>&gt;&gt;&gt; km.fit(X)<br/><br/>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,<br/>n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0)<br/><br/>&gt;&gt;&gt; print(km.cluster_centers_) <br/><br/>[[ 1.39014517, 1.38533993]<br/>[ 9.78473454, 6.1946332 ]<br/>[-5.47807472, 3.73913652]] </pre>
<p><span> </span> <span> </span> <span> </span> <span> </span></p>
<p><span> </span> <span> </span> <span/></p>
<p><span>用三种不同的标记替换数据，我们验证k-means如何成功地分离数据。</span></p>
<p><span><img src="../Images/7020ced9863b249fd30e3be21a066c2f.png" alt="k-means" data-original-src="https://hackr.io/blog/media/k-means-1.png"/>T2】</span></p>
<p><span>在这种情况下，分离是容易的，因为k-means是基于欧几里德距离的，欧几里德距离是径向的，因此聚类预期是凸的。如果所有这些都没有发生，这个问题就不能用这个算法来解决。大多数情况下，即使不完全保证凸性，k-means也可以产生良好的结果，但是在几种情况下，预期的聚类是不可能的，并且让k-means找出质心会导致错误的解决方案。</span></p>

<p><span>让我们也考虑一下同心圆的情况，scikit-learn提供了一个内置函数来生成这样的数据集:</span><span/><span/><span/><span/></p>
<p><span/><span/><span/><span/><span/></p>
<pre>from sklearn.datasets import make_circles<br/>&gt;&gt;&gt; nb_samples = 1000<br/>&gt;&gt;&gt; X, Y = make_circles(n_samples=nb_samples, noise=0.05) </pre>
<p><span>显示同心圆的图形:</span></p>
<p><span><img src="../Images/eecf60529d3881b2c28ce16a25855cfb.png" alt="concentric circles is shown" data-original-src="https://hackr.io/blog/media/concentric-circles-is-shown.png"/>T2】</span></p>
<p>这里我们有一个内部聚类(蓝色三角形标记)和一个外部聚类(红色圆点标记)。这样的集合不是凸的，因此k-means不可能正确地分离它们。</p>
<p><span>假设，我们将算法应用于两个集群:</span></p>
<p><span/><span/><span/><span/><span/></p>
<pre>&gt;&gt;&gt; km = KMeans(n_clusters=2)<br/>&gt;&gt;&gt; km.fit(X)<br/><br/>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,<br/>n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0) </pre>
<p><span>我们得到如下所示的分离:</span></p>
<p><span><img src="../Images/d9803ecd4901823007bd9e551dc5bd8e.png" alt="Graph" data-original-src="https://hackr.io/blog/media/graph.png"/>T2】</span></p>
<p><span>不出所料，k-means收敛在两个半圆中间的两个质心上，得到的聚类截然不同。</span></p>

<h2 id="k-means-clustering-algorithm-code-in-python"><strong> K-means聚类算法的Python代码</strong></h2>
<pre>df = pd.DataFrame({<br/>    'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72],<br/>    'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]<br/>})<br/>from sklearn.cluster import KMeans<br/>kmeans = KMeans(n_clusters=3)<br/>kmeans.fit(df)<br/>------------------------------------------------------------------------------labels = kmeans.predict(df)<br/>centroids = kmeans.cluster_centers_<br/>fig = plt.figure(figsize=(5, 5))<br/><br/>colors = map(lambda x: colmap[x+1], labels)<br/><br/>plt.scatter(df['x'], df['y'], color=colors, alpha=0.5, edgecolor='k')<br/>for idx, centroid in enumerate(centroids):<br/>    plt.scatter(*centroid, color=colmap[idx+1])<br/>plt.xlim(0, 80)<br/>plt.ylim(0, 80)<br/>plt.show()<br/>------------------------------------------------------------------------------</pre>
<h2 id="k-means-clustering-algorithm-code-in-r"><strong>K-表示聚类算法代码中的R </strong></h2>
<pre># K-Means Algorithm <br/>#k=3 # the number of K<br/>max=5000 # the maximum number for generating random points<br/>n=100 # the number of points<br/>maxIter = 10 # maximum number of iterations<br/>threshold = 0.1 #difference of old means and new means<br/># Randomly generate points in the form of (x,y)<br/>x &lt;- sample(1:max, n)<br/>y &lt;- sample(1:max, n)<br/># put point into a matrix<br/>z &lt;- c(x,y)<br/>m = matrix(z, ncol=2)<br/>ks &lt;- c(1,2,4,8,10,15,20) # different Ks<br/>for(k in ks)<br/>   myKmeans(m, k, max)<br/>myKmeans &lt;- function(m, k, max)<br/>{<br/>#initialization for k means: the k-first points in the list<br/>x &lt;- m[, 1]<br/>y &lt;- m[, 2]<br/>d=matrix(data=NA, ncol=0, nrow=0)<br/>for(i in 1:k)<br/>    d &lt;-  c(d, c(x[i], y[i]))<br/>init &lt;- matrix(d, ncol=2, byrow=TRUE)<br/>dev.new()<br/>plotTitle &lt;- paste("K-Means Clustering K = ", k)<br/>plot(m, xlim=c(1,max), ylim=c(1,max), xlab="X", ylab="Y", pch=20, <br/>        main=plotTitle)<br/>par(new=T)<br/>plot(init, pch=2, xlim=c(1,max), ylim=c(1,max), xlab="X", ylab="Y")<br/>par(new=T)<br/>oldMeans &lt;- init<br/>oldMeans <br/>cl &lt;- Clustering(m, oldMeans)<br/>cl<br/>means &lt;- UpdateMeans(m, cl, k)<br/>thr &lt;- delta(oldMeans, means)<br/>itr &lt;- 1<br/>while(thr &gt; threshold)<br/>{<br/>cl &lt;- Clustering(m, means)<br/>oldMeans &lt;- means<br/>means &lt;- UpdateMeans(m, cl, k)<br/>thr &lt;- delta(oldMeans, means)<br/>itr &lt;- itr+1<br/>}<br/>cl<br/>thr<br/>means<br/>itr<br/>for(km in 1:k)<br/>{<br/>   group &lt;- which(cl == km)<br/>   plot(m[group,],axes=F, col=km, xlim=c(1,max), ylim=c(1,max), pch=20, xlab="X", ylab="Y")<br/>   par(new=T)<br/>}<br/>plot(means, axes=F, pch=8, col=15, xlim=c(1,max), ylim=c(1,max), xlab="X", ylab="Y")<br/>par(new=T)<br/>dev.off()<br/>} # end function myKmeans<br/>#function distance<br/>dist &lt;- function(x,y)<br/>{<br/> d&lt;-sqrt( sum((x - y) **2 ))<br/>}<br/>createMeanMatrix &lt;- function(d)<br/>{<br/> matrix(d, ncol=2, byrow=TRUE)<br/>}<br/># compute euclidean distance<br/>euclid &lt;- function(a,b){<br/> d&lt;-sqrt(a**2 + b**2)<br/>}<br/>euclid2 &lt;- function(a){<br/> d&lt;-sqrt(sum(a**2))<br/>}<br/>#compute difference between new means and old means<br/>delta &lt;- function(oldMeans, newMeans)<br/>{<br/> a &lt;- newMeans - oldMeans<br/> max(euclid(a[, 1], a[, 2]))<br/>}<br/>Clustering &lt;- function(m, means)<br/>{<br/>  clusters = c()<br/>  n &lt;- nrow(m)<br/>  for(i in 1:n)<br/>  {<br/>    distances = c()<br/>    k &lt;- nrow(means)<br/>    for(j in 1:k)<br/>    {<br/> di &lt;- m[i,] - means[j,]<br/> ds&lt;-euclid2(di)<br/> distances &lt;- c(distances, ds)<br/>    }<br/>    minDist &lt;- min(distances)<br/>    cl &lt;- match(minDist, distances)<br/>    clusters &lt;- c(clusters, cl)    <br/>  }<br/>  return (clusters)<br/>}<br/>UpdateMeans &lt;- function(m, cl, k)<br/>{<br/> means &lt;- c()<br/> for(c in 1:k)<br/> {<br/>    # get the point of cluster c<br/>    group &lt;- which(cl == c)<br/>    # compute the mean point of all points in cluster c<br/>    mt1 &lt;- mean(m[group,1])<br/>    mt2 &lt;- mean(m[group,2])<br/>    vMean &lt;- c(mt1, mt2)<br/>    means &lt;- c(means, vMean)<br/> }<br/> means &lt;- createMeanMatrix(means)<br/> return(means)<br/>}</pre>
<h2 id="challenges-of-the-k-means-clustering-algorithm"><strong>K-means聚类算法的挑战</strong></h2>
<h3 id="toc-1-different-cluster-size"><strong> 1。不同的集群大小</strong></h3>
<p><span>算法面临的共同挑战是不同的聚类规模。</span></p>

<p><span>让我们用一个例子来理解这一点:</span></p>
<p><span>考虑如下所示的一组原始点:</span></p>
<p><span><img src="../Images/1ac1be1c3f124bdcc7a17cffe5666bca.png" alt="K1-Original Points" data-original-src="https://hackr.io/blog/media/k-1-original-points.png"/>T2】</span></p>
<p><span>在原始图中，与对该算法应用k-means聚类的中心聚类相比，右边和最左边的聚类具有较小的大小，结果将如下所示:</span></p>
<p><span><img src="../Images/22207a4d306c881e6b2a95cadcc00155.png" alt="K-1 2" data-original-src="https://hackr.io/blog/media/k-1-2.png"/>T2】</span></p>
<h3 id="toc-2-different-density-of-data-points"><strong> 2。不同密度的数据点</strong></h3>
<p>当原始点的密度不同时，算法的其他挑战出现。</p>
<p><span>再考虑一下，一组原始点如图:</span></p>
<p><span><img src="../Images/ea1bbcf3ed4e6c83095a1185f5b43f23.png" alt="K-2 1" data-original-src="https://hackr.io/blog/media/k-2-1.png"/>T2】</span></p>
<p><span>在上面的图中，蓝色和给定聚类中的点紧密排列，而红色聚类中的点通过对这些点应用k-means聚类而分散开来。我们将得到如图所示的集群。</span></p>
<p><span><img src="../Images/d58e61661d8594fb72e45486e1e5169d.png" alt="K-2 2" data-original-src="https://hackr.io/blog/media/k-2-2.png"/>T2】</span></p>

<p>我们看到，紧凑的点被分配到一个单独的簇中，而之前分散在同一个簇中的点被分配到不同的簇中。</p>

<p><span>解决方案可以是使用更多的簇，而不是三个簇(k=10 ),从而形成有意义的簇。</span></p>
<h2 id="applications-of-k-means-clustering-algorithm"><strong>K-means聚类算法的应用</strong></h2>
<h3 id="toc-1-document-classification"><strong> 1。文件分类</strong></h3>
<p>这是一个非常标准的分类问题，这个算法被认为适合解决它。基于文档的标签、内容、主题，文档被聚集在多个类别中。</p>
<h3 id="toc-2-customer-segmentation">2.客户细分</h3>
<p>聚类技术根据购买历史、兴趣或活动监控对客户进行细分，从而帮助市场改善其客户基础，在目标领域开展工作。这种分类将有助于公司瞄准特定的客户群。</p>
<h3 id="toc-3-insurance-fraud-detection"><strong> 3。保险欺诈检测</strong></h3>
<p>通过利用欺诈性索赔的历史数据来隔离新的索赔是可能的。基于历史数据，可以形成指示欺诈的聚类。</p>
<h3 id="toc-4-call-record-data-analysis"><strong> 4。通话记录数据分析</strong></h3>
<p>CDR是电信公司获取的信息，用于了解客户群的小时使用情况。</p>
<p>通过电话、短信和互联网收集的信息在与客户的人口统计数据结合使用时，可以更深入地了解客户的需求。</p>
<h3 id="toc-5-cyber-profiling-criminals">5.网络侧写罪犯</h3>
<p>网络特征分析的理念源自犯罪特征分析，并在从个人和团体收集数据的过程中发现重要的相互关系</p>
<p>网络特征分析为调查部门提供信息，以便对犯罪现场的罪犯类型进行分类。</p>
<h2 id="advantages-of-k-means-clustering-algorithm"><strong>K-means聚类算法的优势</strong></h2>
<ol>
<li><span>容易理解。</span></li>
<li><span>鲁棒快速算法。</span></li>
<li><span>复杂度为O(tknd)的高效算法其中:</span></li>
</ol><ul>
<li><span> t:迭代次数。</span></li>
<li>k:质心(簇)的数量。</li>
<li>n:物体的数量。</li>
<li><span> d:每个物体的尺寸。</span></li>
</ul>
<li><span>通常是k，t，d&lt;T3【n】</span></li>
<li>当数据集互不相同且相互分离时，它会给出最佳结果。</li>

<h2 id="disadvantages-of-k-means-clustering-algorithm"><strong>K-means聚类算法的缺点</strong></h2>
<ol>
<li>该算法需要聚类中心数量的先验规范。</li>
<li>如果有两个高度重叠的数据，k-means不能分辨出有两个聚类。</li>
<li>该算法对于非线性变换不是不变的，即不同的数据表示揭示不同的结果。</li>
<li>欧几里得距离度量可以不相等地加权潜在因素。</li>
<li><span>该算法不适用于分类数据，仅在定义平均值时适用。</span></li>
<li><span>无法处理噪音数据和异常值。</span></li>
<li><span>对于非线性数据集，算法失败。</span></li>
</ol>
<p><span><a class="btn btn-primary btn-call-to-action btn-block" href="https://click.linksynergy.com/link?id=jU79Zysihs4&amp;offerid=1045023.2092098&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fmachine-learning-intro-for-python-developers%2F" target="_blank" rel="noopener">Python开发者机器学习入门</a> </span></p>
<h2 id="conclusion"><strong>结论</strong></h2>
<p>这让我们结束了无监督学习算法，k-means聚类。我们已经研究了无监督技术，这是一种<a href="https://hackr.io/blog/types-of-machine-learning">类型的机器学习</a>，其中机器使用未标记的数据进行训练。此外，我们讨论了聚类，简单地说，聚类是将数据集分成由相似数据点组成的组的过程。它有多种用途，最受欢迎的是亚马逊的推荐系统和网飞的电影推荐。接下来，我们学习了我们的博客主题K-means聚类算法，它的算法步骤，并使用一个虚拟数据集来理解它。我们还用Python和T4的代码实现了这个算法。最后，我们研究了该算法面临的挑战及其应用和优缺点。</p>
<p><span>你可以在这里概述更多的机器学习算法<a href="https://hackr.io/blog/machine-learning-algorithms">。</a></span></p>
<p>这些信息对你理解这种算法有帮助吗？让我们知道您的反馈！</p>
<p><strong>人也在读:</strong></p>


									</div>

									</div>    
</body>
</html>