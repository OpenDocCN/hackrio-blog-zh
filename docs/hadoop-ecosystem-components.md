# 15 大 Hadoop 生态系统组件

> 原文：<https://hackr.io/blog/hadoop-ecosystem-components>

大数据是随着时间的推移而积累的数据集的巨大集合，其形式、大小和结构是如此多变，以至于传统形式的 RDBMS 无法有效地处理它。Hadoop 是一个帮助处理这些数据集的框架。它由几个模块组成，这些模块由技术元素的大型生态系统支持。

## 什么是 Hadoop 生态系统？

Hadoop 是基于 Google 的 MapReduce 系统开发的，是基于函数式编程的原理实现的。Hadoop 解决了以下主要问题:

1.  数据存储
2.  数据结构
3.  数据处理

Hadoop 生态系统是一个软件套件，为解决各种大数据问题提供支持。Hadoop 生态系统的核心组件是由各种组织部署的不同服务。生态系统的每个组成部分都有明确的功能。

## Hadoop 生态系统组件

Hadoop 生态系统的不同组件如下:

### 1.Hadoop 分布式文件系统:HDFS

Hadoop 分布式文件系统是 Hadoop 生态系统中最重要的部分。它跨各种节点存储结构化和非结构化数据集，并以日志文件的形式维护元数据。Hadoop 的主要组件包括:

#### 1.1.NameNode

1.  管理和维护 DataNodes(从节点)的是主守护进程。
2.  它记录群集中存储的所有块的元数据[位置、大小、层次结构、权限]。
3.  它记录在文件系统元数据中进行的每一次更改。如果文件被删除，它将立即登录到版本。
4.  它接收来自数据节点的定期心跳，以确保它们仍处于活动状态。
5.  它记录了 HDFS 和 DataNode 中存储数据块的所有数据块。

#### 1.2.DataNode

1.  它是运行在每台从机上的从节点。
2.  这些节点存储实际数据。它将不同格式的输入文件分成块。DataNodes 存储所有这些块。
3.  它负责处理来自客户端的读写请求。
4.  它还负责根据 Namenode 做出的决定创建、删除和复制块。
5.  它每 3 秒钟向 NameNode 发送一次心跳，以报告 HDFS 的整体运行状况。

### 2.MapReduce

它是 Hadoop 的核心数据处理组件。它是一个软件框架，有助于编写在 Hadoop 环境中使用并行和分布式算法处理海量数据集的应用程序。MapReduce 框架负责处理故障。当一个节点出现故障时，它会从另一个节点恢复数据。

在 MapReduce 中，Map()和 Reduce()是两个函数。

1.  map()–该函数执行数据的排序和过滤，并以组的形式组织它们。它接受键值对，并以键值对的形式给出输出。
2.  reduce()–它聚集映射的数据。Reduce()将 Map()生成的输出作为输入，并将它们组合成一个更小的元组集。

### 3.故事

另一个资源协商器 YARN 有助于跨集群管理资源。它为 Hadoop 系统执行调度和资源分配。纱线由两个主要部分组成:

1.  资源管理器:为系统中的应用程序分配资源，并调度 map-reduce 作业。
2.  节点管理器:负责分配资源，如 CPU、内存、每台机器的带宽，并监控它们的使用情况。
3.  应用程序管理器:它充当资源管理器和节点管理器之间的接口，并根据需要执行协商。它还与节点管理器一起监控和执行子任务。

资源调度程序将资源分配给各种正在运行的应用程序。但是，它不监控应用程序的状态。因此，如果出现任何故障，它不会重新启动。

### 4.储备

基于 SQL 方法论和接口，它的查询语言叫做 HQL。它支持所有 SQL 数据类型，这使得查询处理更容易。与查询处理框架类似，HIVE 附带了两个组件:JDBC 驱动程序和 Hive 命令行。JDBC 和 ODBC 驱动一起建立数据存储许可和连接，而 HIVE 命令行帮助处理查询。它执行大型数据集的读写。它允许实时和批处理。

蜂巢的主要组成部分是:

1.  megastore–它存储元数据
2.  驱动因素–管理 HQL 声明的生命周期。
3.  查询编译器–将 HQL 编译成 DAG[有向无环图]
4.  配置单元服务器–为 JDBC/ODBC 服务器提供接口

### 5.猪

PIG 是由雅虎开发的一种查询处理语言，用于查询和分析存储在 HDFS 的数据。PIG 有两个组成部分——PIG Latin 和 Pig Runtime。PIG Latin 有一个类似于 [SQL 命令](https://hackr.io/blog/sql-commands)的结构。MapReduce 作业在 Pig 作业的后端执行。

该清管器的主要特征如下:

1.  可扩展性:允许用户创建自定义功能。
2.  优化机会:自动优化查询，允许用户关注语义而不是效率。
3.  处理各种数据:分析结构化和非结构化数据。

Pig 中的 load 命令加载数据。在后端，编译器将 Pig Latin 转换成一系列 Map-Reduce 作业。可以对数据执行各种功能，如连接、排序、分组和过滤。输出可以转储到屏幕上或存储在 HDFS 文件中。

### 6.HBase

HBase 是建立在 HDFS 之上的 NoSQL 数据库。它支持各种数据。它提供了 Google 大表的功能，因此能够有效地处理大数据集。HBase 是一个开源的、非关系的分布式数据库。它提供对大型数据集的实时读/写访问。这是一个面向列的数据库管理系统。它适用于在大数据用例中非常常见的稀疏数据集。HBase 具有浅延迟存储，企业使用它进行实时分析。HBase 被设计成包含许多表格。每个表都必须有一个主键。

HBase 的各种组件如下:

#### 6.1.HBase 主机

1.  维护和监控 Hadoop 集群。
2.  执行数据库的管理
3.  控制故障转移
4.  仓鼠处理 ddl 操作

#### 6.2 区域服务器

这是一个处理来自客户端的读、写、更新和删除请求的过程。它运行在 Hadoop 集群中的每个节点上，即 HDFS 数据节点。

### 7.象夫

Mahout 为创建可扩展的机器学习应用程序提供了一个平台。它执行协作过滤、聚类和分类。

1.  协同过滤:确定用户行为模式，并在此基础上提出建议。
2.  聚类:它将相似类型的数据组合在一起，如文章、博客、研究论文、新闻等等。
3.  分类:它将数据分类到不同的子部门。
4.  频繁项集缺失:它查找一起购买的项目并给出相应的建议。

### 8.动物园管理员

它协调 Hadoop 生态系统中的各种服务。它与分布式环境中的各种功能相协调。它通过执行同步、配置维护、分组和命名节省了大量时间。Zookeeper 的主要功能如下:

1.  速度:工作负载快。它的读多于写。
2.  组织:它维护所有交易的记录。
3.  简单:它维护一个单一的、分层的名称空间，类似于目录和文件。
4.  可靠:Zookeeper 可以在一组主机上复制，并且所有实例都知道彼此。只要主要服务器可用，动物园管理员就可用。

### 9.驭象者

它是一个用 Java 编写的开源 Web 应用程序。Apache Oozie 是 Hadoop 生态系统中的时钟和闹钟服务。它就像一个作业调度程序。它调度 Hadoop 作业，将它们绑定在一起作为一个逻辑工作。它将多个作业组合成一个工作单元。它可以在 Hadoop 集群中管理数千个工作流。它通过创建工作流的有向无环图来工作。它非常灵活，因为它可以启动、停止、暂停和重新运行失败的作业。

有三种工作:

1.  Oozie 工作流:这些是要执行的一系列动作。
2.  Oozie 协调员:这些是当数据对 it 可用时触发的 oozie 作业。它只对数据的可用性作出反应，否则就休息。
3.  Oozie Bundle:它是许多协调器和工作流任务的包。

### 10.Sqoop

Sqoop 将外部来源的数据导入兼容的 Hadoop 生态系统组件，如 HDFS、Hive、HBase 等。它将数据从 Hadoop 传输到其他外部源。它还可以与 TeraData、Oracle、MySql 等 RDBMS 一起工作。Sqoop 可以处理结构化和非结构化数据。当一个 Sqoop 命令被提交时，它在后端被分成几个子任务，这些子任务就是映射任务。每个 map-task 将数据导入 Hadoop 因此，所有的地图任务集合在一起导入全部数据。Sqoop Export 也以同样的方式工作。这里，map 任务将部分数据从 Hadoop 导出到目标数据库。

### 11.水道

Flume 是一个分布式的、可靠的、可用的服务，用于有效地收集、聚合和移动来自各种 web 服务器的大量流数据到 HDFS。Flume 有三个组件:

1.  Source:它接受来自传入流的数据，并将数据存储在通道中。
2.  通道:它是数据源和临时存储之间的临时存储介质。
3.  接收器:该组件从通道收集数据，并将其永久写入 HDFS。

### 12.安巴里

它负责调配、管理、监控和保护 Hadoop 集群。Ambari 的不同特点如下:

1.  简化的群集配置、管理和安装。
2.  降低 Hadoop 集群安全配置和管理的复杂性
3.  定义在 Hadoop 集群上安装 Hadoop 服务的分步过程。
4.  处理 Hadoop 集群中的服务配置。
5.  仪表板可用于集群监控。
6.  当节点关闭或磁盘空间不足时，琥珀色警报框架会生成警报。

### 13.阿帕奇演习

它是一个无模式的 SQL 查询引擎。它是一种分布式查询处理语言。它适用于 Hadoop、NoSQL 和云存储。它的主要目的是低延迟的大规模数据处理。以下是 Apache Drill 的主要功能:

1.  能够扩展数千个节点。
2.  支持 NoSQL 数据库，如 Azure BLOB storage、Google Cloud Storage、亚马逊 S3、HBase、MongoDB 等
3.  单个查询可以基于各种数据库。
4.  支持数百万用户，并在大型数据集上为他们的查询提供服务。
5.  提供更快的洞察力，没有加载、模式创建、维护、转换等 ETL 开销
6.  分析多结构和嵌套数据，无需转换或过滤。

### 14.阿帕奇火花

它统一了各种大数据处理。Spark 内置了用于流、SQL、机器学习和图形处理的库。Apache Spark 为批处理和流处理提供了闪电般的性能。这是在 DAG 调度器、查询优化器和物理执行引擎的帮助下完成的。

1.  Spark 可以在 Hadoop、Mesos 或 Kubernetes 上以独立集群模式运行。
2.  Spark 应用程序可以使用 SQL、R、Python、Scala 和 Java 编写。
3.  Spark 提供了 80 个高级操作符，这使得构建并行应用程序变得很容易。
4.  它有各种各样的图书馆，比如

1.  机器学习的 MLlib
2.  用于图形处理的 GraphX
3.  SQL、数据帧和 Spark 流

6.  Spark 执行内存处理，这使得它比 Hadoop Map-Reduce 更快。

### 15.索尔-卢斯内公司

Apache Solr 和 Apache Lucene 是搜索和索引 Hadoop 生态系统的两个服务。Apache Solr 是围绕 Apache Lucene 构建的。Apache Lucene 内置 Java，使用 Java 库进行搜索和索引。Apache Solr 是一个开源搜索平台。Apache Solr 的不同特性如下:

1.  Solr 具有高度的可伸缩性、可靠性和容错性。
2.  它提供了

1.  分布式索引
2.  自动化故障转移和恢复
3.  负载平衡查询
4.  集中式配置

4.  可以使用 HTTP GET 生成查询，并接收 JSON、二进制、CSV 和 XML 格式的结果。
5.  它提供了匹配功能，如短语、通配符、分组、连接等等。
6.  它有一个内置的管理界面，支持 Solr 实例的管理。
7.  Solr 利用了 Lucene 的实时索引。因此，它使用户能够在任何你想看的时候看到内容。

[Hadoop 的终极实践:驯服您的大数据！](https://click.linksynergy.com/link?id=jU79Zysihs4&offerid=1045023.996228&type=2&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fthe-ultimate-hands-on-hadoop-tame-your-big-data%2F)

## 结论

Hadoop 生态系统的所有元素都是开放系统 Apache Hadoop 项目。

1.  核心是用于数据存储的 HDFS、用于数据处理的 Map-Reduce 和作为资源管理器的 YARN。
2.  HIVE 是一个数据分析工具
3.  PIG 是一种类似脚本语言的 SQL。
4.  h base–NoSQL 数据库
5.  一个机器学习工具
6.  zookeeper——同步工具
7.  oo zie–工作流调度系统
8.  结构化数据导入和导出工具。
9.  flume——一个用于非结构化和半结构化数据的数据传输工具
10.  ambari——管理和保护 Hadoop 集群的工具

一旦你清楚了以上的概念，你就可以认为你已经准备好进一步学习这个领域的知识了。