<html>
<head>
<title>Decision Tree in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>机器学习中的决策树</h1>
<blockquote>原文：<a href="https://hackr.io/blog/decision-tree-in-machine-learning#0001-01-01">https://hackr.io/blog/decision-tree-in-machine-learning#0001-01-01</a></blockquote><div><div class="content">
										<h2 id="what-is-a-decision-tree">什么是决策树？</h2>
<p>决策树采用树状结构，作为预测模型，来明确地表示决策和决策制定。决策树的每个内部节点都是一个特征，该节点的每个输出边都代表该特征可以取的值。</p>
<p>对于分类要素，输出边的数量是这些类别中不同值的数量。在数字特征的情况下，输出边的数量通常是两个，其中一个特征值小于实数值，另一个大于实数值。每个叶节点代表一个类标签。基于信息增益来选择每个节点处的特征，并且具有最大增益的特征更重要，并且在更高的级别(更靠近根节点)被选择。</p>
<h2 id="building-a-decision-tree">构建决策树</h2>
<p>我们将通过一个例子来学习如何建立一个决策树。</p>
<p>你的约会对象是好还是坏的精神病患者？</p>

<p>在这个例子中，一个数据集定义了超级英雄的属性，然后基于训练，模型评估你的约会对象是好还是坏的超级英雄。考虑下面的数据集。</p>
<table>
<tbody>
<tr>
<td><strong>超级英雄</strong></td>
<td><strong>屏蔽</strong></td>
<td><strong>海角</strong></td>
<td><strong>平局</strong></td>
<td><strong>耳朵</strong></td>
<td><strong>吸烟</strong></td>
<td><strong>高度</strong></td>
<td><strong>类</strong></td>
</tr>
<tr>
<td><strong>蝙蝠侠</strong></td>
<td>y</td>
<td>y</td>
<td>n</td>
<td>y</td>
<td>n</td>
<td>180</td>
<td>好的</td>
</tr>
<tr>
<td><strong>罗宾</strong></td>
<td>y</td>
<td>y</td>
<td>n</td>
<td>n</td>
<td>n</td>
<td>176</td>
<td>好的</td>
</tr>
<tr>
<td>阿尔弗雷德</td>
<td>n</td>
<td>n</td>
<td>y</td>
<td>n</td>
<td>n</td>
<td>185</td>
<td>好的</td>
</tr>
<tr>
<td><strong>企鹅</strong></td>
<td>n</td>
<td>n</td>
<td>y</td>
<td>n</td>
<td>y</td>
<td>140</td>
<td>邪恶</td>
</tr>
<tr>
<td><strong>猫女</strong></td>
<td>y</td>
<td>n</td>
<td>n</td>
<td>y</td>
<td>n</td>
<td>170</td>
<td>邪恶</td>
</tr>
<tr>
<td><strong>小丑</strong></td>
<td>n</td>
<td>n</td>
<td>n</td>
<td>n</td>
<td>n</td>
<td>179</td>
<td>邪恶</td>
</tr>
<tr>
<td>蝙蝠女孩</td>
<td>y</td>
<td>y</td>
<td>n</td>
<td>y</td>
<td>n</td>
<td>165</td>
<td>？</td>
</tr>
<tr>
<td><strong>谜语者</strong></td>
<td>y</td>
<td>n</td>
<td>n</td>
<td>n</td>
<td>n</td>
<td>182</td>
<td>？</td>
</tr>
<tr>
<td>你的约会</td>
<td>n</td>
<td>y</td>
<td>y</td>
<td>y</td>
<td>y</td>
<td>181</td>
<td>？</td>
</tr>
</tbody>
</table>
<p>涵盖所有方面和测试用例的一个可能的决策树是:</p>
<p><img src="../Images/0fc828d03188feb760e22ddae967a7ec.png" data-original-src="https://hackr.io/blog/media/decision-tree-1.png"/></p>
<p>不建议构建和分割树并使其变得复杂，因为虽然它给出了0%的测试误差，但它会导致过度拟合。所以，我们的目标是找到最小的树来得到训练好的数据集。</p>
<p>现在让我们看看构建高效决策树的伪代码。</p>
<p>伪代码如下:</p>

<h3 id="procedure-buildsubtree-set-of-training-instances-d"><strong>过程</strong>构建子树(训练实例集D)</h3>
<ol>
<li>   <ol>
<li>C </li>
<li><strong>如果</strong>停止标准满足<strong>，则</strong></li>
<li>制作叶节点N</li>
</ol>
</li>
</ol>
<p>确定N的类别标签</p>
<ol>
<ol>
<li><strong>否则</strong></li>
</ol>
</ol>
<h3 id="else">制作内部节点N</h3>
<ol>
<li>S <ol>
<li>对于S的每个结果o</li>
<li>Do </li>
<li>N </li>
<li>返回以N为根的子树</li>
<li>所以接下来最小的树可能是:</li>
<li><img src="../Images/00a339d36c8e7ef8d2e1b0897d44cee5.png" data-original-src="https://hackr.io/blog/media/smallest-tree.png"/></li>
</ol>
</li>
</ol>
<p>决策树不是一个有效的算法，但是，它们有明确的偏差-方差问题，变得容易解决这些问题。方差可通过装袋进行调整，偏差可通过助推进行调整。</p>
<p>熵</p>
<p>熵是与随机变量y相关的不确定性的度量。它是传达变量值所需的预期位数。</p>
<h2 id="entropy">其中P(y)是Y具有值Y的概率。关于决策树，熵用于寻找在任何节点的最佳特征分裂。</h2>
<p>不同决策树算法使用的拆分规则</p>
<pre><img src="../Images/96739568adc29757f46cbc260904a739.png" data-original-src="https://hackr.io/blog/media/entropy.png"/></pre>
<p>1.信息增益</p>
<h2 id="splitting-rules-used-by-different-decision-tree-algorithms">信息增益是熵H的变化，当在特征上分裂时，从先前状态到新状态:</h2>
<h3 id="toc-1-information-gain">信息增益用于识别分割给定训练数据集的最佳特征。它选择最能减少训练数据集d的输出Y的条件熵的分裂S</h3>
<p>计算所有特征的信息增益，并选择具有最高增益的特征作为最重要的特征。</p>
<pre><img src="../Images/4145e5d86aa4494498613a4191c3b100.png" data-original-src="https://hackr.io/blog/media/information-gain-1.png"/></pre>
<p>2.增益比</p>
<p>增益比将信息增益除以所考虑的分裂的熵进行归一化，从而避免信息增益的不合理偏好。</p>
<h3 id="toc-2-gain-ratio">3.排列测试</h3>
<p>有机数据的标记被置换，并且对于标记的所有可能置换，为重新标记的数据计算要测试的统计量。原始数据的测试统计值与通过所有排列获得的值进行比较，通过计算后者的百分比，后者至少与前者一样极端。</p>
<pre><img src="../Images/424214ccdce76ee698cdd7856eaa80fe.png" data-original-src="https://hackr.io/blog/media/gain-ratio.png"/></pre>
<h3 id="toc-3-permutation-test"><strong>建议课程</strong></h3>
<p><a class="btn btn-primary btn-call-to-action " href="https://click.linksynergy.com/deeplink?id=jU79Zysihs4&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fcomplete-machine-learning-and-data-science-zero-to-mastery%2F" target="_blank" rel="noopener">完全机器学习&amp;数据科学训练营2023 </a></p>
<p>4.多元分裂</p>
<p>多元决策树可以使用在每个内部节点包含多个属性的split。</p>
<h3 id="toc-4-multivariate-split">5.杂质函数和基尼指数</h3>
<p><strong>杂质函数:</strong>测量标签纯度的函数。</p>
<h3 id="toc-5-impurity-function-and-gini-index"><strong>基尼杂质:</strong>对于一组数据点S，</h3>
<p><img src="../Images/0181b6cd9239ca4f8af011342b68e48a.png" data-original-src="https://hackr.io/blog/media/gini.png"/></p>
<p>用某个标签选择一个点的概率</p>
<p><img src="../Images/b7442edee05873c2dcfe28692792edba.png" data-original-src="https://hackr.io/blog/media/picking-a-point.png"/></p>
<p><img src="../Images/4157bf24ed1085921aa1d2dc954094be.png" data-original-src="https://hackr.io/blog/media/gini-impurity.png"/></p>
<p><strong>基尼指数:</strong>这是一个衡量从数据集中随机选择的实例被错误标记的频率，如果它是根据子集中标签的分布随机标记的。</p>
<p>修剪</p>
<p>修剪是一种通过从最终分类器中移除子树来降低最终分类器复杂性的技术，子树的存在不会影响模型的准确性。在修剪中，你生长出完整的树，然后反复修剪掉一些节点，直到进一步修剪是有害的。这是通过评估修剪每个节点对调整(验证)数据集准确性的影响，并贪婪地删除最能提高数据准确性的节点来实现的。</p>
<h2 id="pruning">可以通过对到达叶子的训练样本的数量施加最小值来修剪决策树。修剪使树保持简单，而不影响整体的准确性。它通过减小树的大小和复杂性来帮助解决过拟合问题。</h2>
<p>密码</p>
<p><strong>输出:</strong></p>
<h3 id="code">为什么要使用决策树？决策树的优势</h3>
<pre>def main(name):<br/>for i in range(len(date_data)):<br/>superhero_data = dict(zip(header,date_data[i]))<br/>if superhero_data["Superhero"].lower()==name.lower():<br/>break<br/>return smoke(superhero_data)<br/>def smoke(superhero_data):<br/>"To check whether the Superhero Smoke or Not"<br/>if superhero_data["Smoker"]=="y":<br/>return ears(superhero_data)<br/>else:<br/>return mask(superhero_data)<br/>def ears(superhero_data):<br/>if superhero_data["Ears"]=="y":<br/>return "Good"<br/>else:<br/>return "Evil"<br/>def mask(superhero_data):<br/>if superhero_data['Mask']=="y":<br/>return height(superhero_data)<br/>else:<br/>return tie(superhero_data)<br/>def height(superhero_data):<br/>if superhero_data["Height"]&gt;175:<br/>return"Good"<br/>else:<br/>return "Evil"<br/>def tie(superhero_data):<br/>if superhero_data["Tie"]=="y":<br/>return "Good"<br/>else:<br/>return "Evil"<br/>if __name__=="__main__":<br/>date_data = [ ["Batman","y","y","n","y","n",180],<br/>["Robin","y","y","n","n","n",176],<br/>["Alfred","n","n","y","n","n",185],<br/>["Penguin","n","n","y","n","y",140],<br/>["Catwomen","y","n","n","y","n",170],<br/>["Joker","n","n","n","n","n",179],<br/>["Batgirl","y","y","n","y","n",165],<br/>["Riddler","y","n","n","n","n",182],<br/>["Your Date","n","y","y","y","y",181],<br/>]<br/>header=["Superhero", "Mask", "Cape", "Tie", "Ears", "Smoker", "Height", "Class"]<br/>print("-----Is your date a good or bad psychopath---- ")<br/>for i in range(len(date_data)):<br/>print(date_data[i][0],"is ---&gt;",end="")<br/>print(main(date_data[i][0]))</pre>
<p>当我们将决策树拟合到训练数据集时，拆分决策树的前几个节点基本上是数据集中最重要的特征，因此，您可以将其用作特征选择技术来选择数据集中最相关的特征。决策树对异常值也不敏感，因为分裂是基于分裂范围内样本的比例而不是绝对值发生的。</p>
<pre>-----Is your date a good or bad psychopath----<br/>Batman is ---&gt;Good<br/>Robin is ---&gt;Good<br/>Alfred is ---&gt;Good<br/>Penguin is ---&gt;Evil<br/>Catwoman is ---&gt;Evil<br/>Joker is ---&gt;Evil<br/>Batgirl is ---&gt;Evil<br/>Riddler is ---&gt;Good<br/>Your Date is ---&gt;Good</pre>
<h2 id="why-use-a-decision-tree-advantages-of-a-decision-tree">由于它们的树状结构，它们非常容易理解和解释。它们不需要对数据进行归一化，并且即使当特征彼此之间具有非线性关系时也能很好地工作。</h2>
<p>使用决策树算法的缺点</p>
<p>即使是输入数据中的小变化，有时也会导致树中的大变化，因为它可能会极大地影响决策树用来选择特征的信息增益。</p>

<h3 id="disadvantages-of-using-a-decision-tree-algorithm">此外，决策树一次只检查一个字段，导致矩形分类框。这可能与决策空间中记录的实际分布不太一致。</h3>
<ol>
<li>当涉及到应用回归和预测连续值时，决策树是不够的。一个连续变量在一个区间内可以有无限多的值，在一个只有有限数量的分支和叶子的树中，捕捉这些值是非常困难的。</li>
<li>相同的子树在不同的路径上有可能重复，导致复杂的树。</li>
<li>树中的每个特征都被迫与树中更高层的每个特征进行交互。如果有些特性没有交互或者交互很弱，那么这是非常低效的。</li>
<li>结论</li>
<li>决策树是一个NP-hard问题，即如果数据集变大，容纳额外数据所需的计算时间将呈指数增长。其目的是找到正确训练系统的最小的树。我们希望你理解这个机器学习的概念。你可能有兴趣在这里了解更多的机器学习算法。</li>
</ol>
<h2 id="conclusion">你最喜欢的机器学习概念是什么？让我们在下面评论吧！</h2>
<p><strong>人也在读:</strong></p>
<p>Which is your favourite machine learning concept? Let us in the comments below!</p>
<p><strong>People are also reading:</strong></p>


									</div>

									</div>    
</body>
</html>