# 分类算法简介[类型]

> 原文：<https://hackr.io/blog/classification-algorithm>

## **分类算法是什么？**

我们在分析部分的任务是从了解目标类的步骤开始的。所以这整个过程被说成是分类。算法是用于解决数学和计算机科学问题的程序或公式，它基于按指定动作的顺序执行步骤。我们可以把计算机程序看作一个详细的算法。我们在几乎所有的信息技术中都使用算法。

例如，考虑到搜索引擎算法，它搜索键盘字符串。它作为输入操作，分别与相关的网页相关联并给我们结果。像美国国防部数据加密标准(des)那样的加密算法使用秘密密钥算法来保护数据免受黑客攻击或病毒传播，因为国家信息的泄露会使他们处于危险之中。只要算法被充分引用，没有密钥的人就无法解密受保护的数据。

### **目标类的一些例子**

对购买者数据进行分析，以预测他是否会购买计算机配件(目标类别:是或否)

根据颜色、味道、大小、重量对水果进行分组和区分(目标类别:苹果、芒果、荔枝、樱桃、木瓜、橙子、甜瓜和番茄)

从头发长度区分性别(目标类别:男性或女性)

现在，我们将理解分类算法的概念，根据他们的性别——基于他们的头发长度——来区分他们(决不是我试图按性别套用，这只是为了举例)。我们应该有合适的头发长度值。让我们假设识别边界头发长度值是 25.0 厘米；然后，我们说头发的长度比这长，那么性别可能是男性或女性。

### **数据集来源和内容**

数据集包含工资。以下是我们数据集的描述:

```
Of classes: 2(">50k" and "<=50k")
Of attributes (columns): 7
Of instances (rows): 48,842
```

这个数据来自人口普查局的数据库。

**解释**

考虑了两类工资。第一个大于 50k，第二个等于且小于 50k。如果我们取 7 个属性或 48，842 个列和行，考虑来自人口普查局数据库的数据，那么我们可以很容易地将具有 7 个属性的人的名字分布在初始阶段考虑的两个组工资下。因此，使用这种方法可以避免一些计算和劳动。

### 分类算法的应用

*   垃圾邮件分类
*   银行客户贷款支付意愿预测
*   癌症肿瘤细胞鉴定
*   情感分析
*   药品分类
*   面部关键点检测
*   汽车行驶中的行人检测

### **分类算法的类型**

分类算法可以大致分为以下几类:

*   线性分类器
*   逻辑回归
*   朴素贝叶斯分类器
*   费希尔线性判别式
*   支持向量机
*   最小二乘支持向量
*   二次分类
*   核估计
*   k-最近邻
*   决策树
*   随机森林
*   神经网络
*   学习矢量量化

## **解释一些重要的分类算法类型**

### **1。逻辑回归**

逻辑回归是一种分类，而不是回归算法。

#### **R 代码**

```
X< - cbind (x_train, y_train)
# train the model using the training sets and check score logistic < - glm (y_train - ., data = x, family ="binomial")
Summary (logistic)
# predict output
```

预测=预测(logistic，x_test)

有许多步骤可以帮助我们改进模型:

*   包括交互术语
*   移除功能
*   规范技术
*   使用非线性模型

**优势**

*   它是为分类而设计的，对于理解一些独立变量对单个结果变量的影响最有用。

**缺点**

*   只有当预测变量是二进制时，它才起作用。

**建议课程**

[掌握编码面试:数据结构+算法](https://click.linksynergy.com/deeplink?id=jU79Zysihs4&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fmaster-the-coding-interview-data-structures-algorithms%2F)

### 2.**决策树**

[决策树](https://hackr.io/blog/decision-tree-in-machine-learning)支持使用分类问题的监督学习算法。

#### **R 代码**

```
Library (rpart)
X < - cbind (x_train, y_train)
# grow tree
Fit < - rpart (y_train - ., data = x, method="class")
Summary (fit)
# predict output
Predicted = predict (fit, x_test)
```

**优势**

*   决策树易于理解和可视化，需要很少的数据准备，并且可以处理数值和分类数据。

**缺点**

*   它可以被创建为复杂的树。

### **3。朴素贝叶斯分类器**

它考虑了预测器之间的独立性假设，即所谓的[贝叶斯定理](https://hackr.io/blog/top-steps-to-learn-naive-bayes-algorithm)。

它帮助我们从 p(c)，p(x)和 p(x0/c)计算后验概率 p(c/x)

```
P(c/x) = (p(x/c) p(c)) / p(x)
```

这里，

P(c/x)是类(目标)状态预测器(属性)的后验概率。

**举例:**

现在我们将根据球员是否参加比赛的天气来分类。

*   **第一步:**首先，我们要把数据集转换成频率表。
*   **步骤 2:** 现在，我们必须创建一个可能性表，找到阴概率= 0.29，玩的概率是 0.64
*   **第三步:**在第二步之后，我们必须使用朴素贝叶斯方程计算每一类的后验概率。

**举例:**

如果天气晴朗，一个高尔夫球员会去打球。这种说法正确吗？

我们将通过以下方式解方程

p(是/晴朗)=p(晴朗/是)*p(是)/p(晴朗)

现在，p(晴/是)=3/9 =0.33，

P(晴)=5/14 =0.36，
P(是)=9/14 =0.64。

现在，p(是/晴)=0.33*0.64/0.36 =0.60

这个概率比较大。

#### **R 代码**

```
Library (e1071)
X < - cbind (x_train, y_train)
# fitting model
Fit < -naivebayes (y_train - ., data= summary (fit)
#predict output
Predicted = predict (fit, x_test)
```

**优势**

*   这种类型的算法需要少量的训练数据来估计所需的参数。
*   与更通用的方法相比，这种方法速度非常快。

**缺点**

*   它不能做出很好的估计。

### **4。SVM(支持向量机)**

它有助于协调具有不同特征的组。例如，如果我们只有两个特征，如个人的身高和头发长度，首先，我们必须绘制这些二维空间，其中每个点有两个坐标，这就是所谓的支持向量。

#### **R 代码**

```
Library (e1071)
X < - cbind (x_train, y_train)
# fitting model
Fit < - svm (y_train - ., data = x)
Summary (fit)
#predict output
Predicted = predict (fit, x_test)
```

**优势**

在高维空间中很好，并且在决策函数中使用训练点的子集，所以它也是内存高效的。

**缺点**

它给出了可能难以理解和分析的复杂结果。

它将根据用户的指示考虑任何组，即使它们是不相关的。

### **5。随机梯度下降**

当样本量较大时，使用随机梯度下降。

#### **R 代码**

从 sklearn.linear_model 导入 sgdclassifier

```
Sgd = sgdclassifier (loss = "modified_huber", shuffle = true, random_state =101)
Sgd.fit (x_train, y_train)
Y_pred = sgd.predict (x_test)
```

**优势**

*   高效且易于实施。

**缺点**

*   它需要几个超参数，并且对特征缩放很敏感。

## **结论**

分类算法有助于对买家数据进行无效分析，以预测他是否会购买电脑配件。它还有助于对项目进行分组并区分输入，从而节省大量的时间和精力。因此，分析变得更加容易，分类支持过程加快了决策过程，这对于在竞争激烈的世界中保持业务的持续发展至关重要。

对数学思维感兴趣吗？我们推荐陶哲轩的这个[大师班课程。他解决常见问题的方法也可以应用于更复杂的分类算法。或者为数据科学项目尝试这些](https://masterclass.pxf.io/c/2890636/1357540/16021)[数据营课程](https://datacamp.pxf.io/DVGE65)。

**人也在读:**