<html>
<head>
<title>15 Top Hadoop Ecosystem Components</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>15大Hadoop生态系统组件</h1>
<blockquote>原文：<a href="https://hackr.io/blog/hadoop-ecosystem-components#0001-01-01">https://hackr.io/blog/hadoop-ecosystem-components#0001-01-01</a></blockquote><div><div class="content">
										<p>大数据是随着时间的推移而积累的数据集的巨大集合，其形式、大小和结构是如此多变，以至于传统形式的RDBMS无法有效地处理它。Hadoop是一个帮助处理这些数据集的框架。它由几个模块组成，这些模块由技术元素的大型生态系统支持。</p>
<h2 id="what-is-hadoop-ecosystem">什么是Hadoop生态系统？</h2>
<p>Hadoop是基于Google的MapReduce系统开发的，是基于函数式编程的原理实现的。Hadoop解决了以下主要问题:</p>
<ol>
<li>数据存储</li>
<li>数据结构</li>
<li>数据处理</li>
</ol>
<p>Hadoop生态系统是一个软件套件，为解决各种大数据问题提供支持。Hadoop生态系统的核心组件是由各种组织部署的不同服务。生态系统的每个组成部分都有明确的功能。</p>
<h2 id="hadoop-ecosystem-components">Hadoop生态系统组件</h2>
<p>Hadoop生态系统的不同组件如下:</p>

<h3 id="toc-1-the-hadoop-distributed-file-system-hdfs">1.Hadoop分布式文件系统:HDFS</h3>
<p>Hadoop分布式文件系统是Hadoop生态系统中最重要的部分。它跨各种节点存储结构化和非结构化数据集，并以日志文件的形式维护元数据。Hadoop的主要组件包括:</p>
<h4 id="toc-1-1-namenode">1.1.NameNode</h4>
<ol>
<li>管理和维护DataNodes(从节点)的是主守护进程。</li>
<li>它记录群集中存储的所有块的元数据[位置、大小、层次结构、权限]。</li>
<li>它记录在文件系统元数据中进行的每一次更改。如果文件被删除，它将立即登录到版本。</li>
<li>它接收来自数据节点的定期心跳，以确保它们仍处于活动状态。</li>
<li>它记录了HDFS和DataNode中存储数据块的所有数据块。</li>
</ol>
<h4 id="toc-1-2-datanode">1.2.DataNode</h4>
<ol>
<li>它是运行在每台从机上的从节点。</li>
<li>这些节点存储实际数据。它将不同格式的输入文件分成块。DataNodes存储所有这些块。</li>
<li>它负责处理来自客户端的读写请求。</li>
<li>它还负责根据Namenode做出的决定创建、删除和复制块。</li>
<li>它每3秒钟向NameNode发送一次心跳，以报告HDFS的整体运行状况。</li>
</ol>
<h3 id="toc-2-mapreduce">2.MapReduce</h3>
<p>它是Hadoop的核心数据处理组件。它是一个软件框架，有助于编写在Hadoop环境中使用并行和分布式算法处理海量数据集的应用程序。MapReduce框架负责处理故障。当一个节点出现故障时，它会从另一个节点恢复数据。</p>
<p>在MapReduce中，Map()和Reduce()是两个函数。</p>
<ol>
<li>map()–该函数执行数据的排序和过滤，并以组的形式组织它们。它接受键值对，并以键值对的形式给出输出。</li>
<li>reduce()–它聚集映射的数据。Reduce()将Map()生成的输出作为输入，并将它们组合成一个更小的元组集。</li>
</ol>
<h3 id="toc-3-yarn">3.故事</h3>
<p>另一个资源协商器YARN有助于跨集群管理资源。它为Hadoop系统执行调度和资源分配。纱线由两个主要部分组成:</p>
<ol>
<li>资源管理器:为系统中的应用程序分配资源，并调度map-reduce作业。</li>
<li>节点管理器:负责分配资源，如CPU、内存、每台机器的带宽，并监控它们的使用情况。</li>
<li>应用程序管理器:它充当资源管理器和节点管理器之间的接口，并根据需要执行协商。它还与节点管理器一起监控和执行子任务。</li>
</ol>
<p>资源调度程序将资源分配给各种正在运行的应用程序。但是，它不监控应用程序的状态。因此，如果出现任何故障，它不会重新启动。</p>
<h3 id="toc-4-hive">4.储备</h3>
<p>基于SQL方法论和接口，它的查询语言叫做HQL。它支持所有SQL数据类型，这使得查询处理更容易。与查询处理框架类似，HIVE附带了两个组件:JDBC驱动程序和Hive命令行。JDBC和ODBC驱动一起建立数据存储许可和连接，而HIVE命令行帮助处理查询。它执行大型数据集的读写。它允许实时和批处理。</p>

<p>蜂巢的主要组成部分是:</p>
<ol>
<li>megastore–它存储元数据</li>
<li>驱动因素–管理HQL声明的生命周期。</li>
<li>查询编译器–将HQL编译成DAG[有向无环图]</li>
<li>配置单元服务器–为JDBC/ODBC服务器提供接口</li>
</ol>
<h3 id="toc-5-pig">5.猪</h3>
<p>PIG是由雅虎开发的一种查询处理语言，用于查询和分析存储在HDFS的数据。PIG有两个组成部分——PIG Latin和Pig Runtime。PIG Latin有一个类似于<a href="https://hackr.io/blog/sql-commands"> SQL命令</a>的结构。MapReduce作业在Pig作业的后端执行。</p>
<p>该清管器的主要特征如下:</p>
<ol>
<li>可扩展性:允许用户创建自定义功能。</li>
<li>优化机会:自动优化查询，允许用户关注语义而不是效率。</li>
<li>处理各种数据:分析结构化和非结构化数据。</li>
</ol>
<p>Pig中的load命令加载数据。在后端，编译器将Pig Latin转换成一系列Map-Reduce作业。可以对数据执行各种功能，如连接、排序、分组和过滤。输出可以转储到屏幕上或存储在HDFS文件中。</p>
<h3 id="toc-6-hbase">6.HBase</h3>
<p>HBase是建立在HDFS之上的NoSQL数据库。它支持各种数据。它提供了Google大表的功能，因此能够有效地处理大数据集。HBase是一个开源的、非关系的分布式数据库。它提供对大型数据集的实时读/写访问。这是一个面向列的数据库管理系统。它适用于在大数据用例中非常常见的稀疏数据集。HBase具有浅延迟存储，企业使用它进行实时分析。HBase被设计成包含许多表格。每个表都必须有一个主键。</p>
<p>HBase的各种组件如下:</p>
<h4 id="toc-6-1-hbase-master">6.1.HBase主机</h4>
<ol>
<li>维护和监控Hadoop集群。</li>
<li>执行数据库的管理</li>
<li>控制故障转移</li>
<li>仓鼠处理ddl操作</li>
</ol>
<h4 id="toc-6-2-region-server">6.2区域服务器</h4>
<p>这是一个处理来自客户端的读、写、更新和删除请求的过程。它运行在Hadoop集群中的每个节点上，即HDFS数据节点。</p>
<h3 id="toc-7-mahout">7.象夫</h3>
<p>Mahout为创建可扩展的机器学习应用程序提供了一个平台。它执行协作过滤、聚类和分类。</p>
<ol>
<li>协同过滤:确定用户行为模式，并在此基础上提出建议。</li>
<li>聚类:它将相似类型的数据组合在一起，如文章、博客、研究论文、新闻等等。</li>
<li>分类:它将数据分类到不同的子部门。</li>
<li>频繁项集缺失:它查找一起购买的项目并给出相应的建议。</li>
</ol>
<h3 id="toc-8-zookeeper">8.动物园管理员</h3>
<p>它协调Hadoop生态系统中的各种服务。它与分布式环境中的各种功能相协调。它通过执行同步、配置维护、分组和命名节省了大量时间。Zookeeper的主要功能如下:</p>
<ol>
<li>速度:工作负载快。它的读多于写。</li>
<li>组织:它维护所有交易的记录。</li>
<li>简单:它维护一个单一的、分层的名称空间，类似于目录和文件。</li>
<li>可靠:Zookeeper可以在一组主机上复制，并且所有实例都知道彼此。只要主要服务器可用，动物园管理员就可用。</li>
</ol>
<h3 id="toc-9-oozie">9.驭象者</h3>
<p>它是一个用Java编写的开源Web应用程序。Apache Oozie是Hadoop生态系统中的时钟和闹钟服务。它就像一个作业调度程序。它调度Hadoop作业，将它们绑定在一起作为一个逻辑工作。它将多个作业组合成一个工作单元。它可以在Hadoop集群中管理数千个工作流。它通过创建工作流的有向无环图来工作。它非常灵活，因为它可以启动、停止、暂停和重新运行失败的作业。</p>
<p>有三种工作:</p>
<ol>
<li>Oozie工作流:这些是要执行的一系列动作。</li>
<li>Oozie协调员:这些是当数据对it可用时触发的oozie作业。它只对数据的可用性作出反应，否则就休息。</li>
<li>Oozie Bundle:它是许多协调器和工作流任务的包。</li>
</ol>
<h3 id="toc-10-sqoop">10.Sqoop</h3>
<p>Sqoop将外部来源的数据导入兼容的Hadoop生态系统组件，如HDFS、Hive、HBase等。它将数据从Hadoop传输到其他外部源。它还可以与TeraData、Oracle、MySql等RDBMS一起工作。Sqoop可以处理结构化和非结构化数据。当一个Sqoop命令被提交时，它在后端被分成几个子任务，这些子任务就是映射任务。每个map-task将数据导入Hadoop因此，所有的地图任务集合在一起导入全部数据。Sqoop Export也以同样的方式工作。这里，map任务将部分数据从Hadoop导出到目标数据库。</p>
<h3 id="toc-11-flume">11.水道</h3>
<p>Flume是一个分布式的、可靠的、可用的服务，用于有效地收集、聚合和移动来自各种web服务器的大量流数据到HDFS。Flume有三个组件:</p>
<ol>
<li>Source:它接受来自传入流的数据，并将数据存储在通道中。</li>
<li>通道:它是数据源和临时存储之间的临时存储介质。</li>
<li>接收器:该组件从通道收集数据，并将其永久写入HDFS。</li>
</ol>
<h3 id="toc-12-ambari">12.安巴里</h3>
<p>它负责调配、管理、监控和保护Hadoop集群。Ambari的不同特点如下:</p>
<ol>
<li>简化的群集配置、管理和安装。</li>
<li>降低Hadoop集群安全配置和管理的复杂性</li>
<li>定义在Hadoop集群上安装Hadoop服务的分步过程。</li>
<li>处理Hadoop集群中的服务配置。</li>
<li>仪表板可用于集群监控。</li>
<li>当节点关闭或磁盘空间不足时，琥珀色警报框架会生成警报。</li>
</ol>
<h3 id="toc-13-apache-drill">13.阿帕奇演习</h3>
<p>它是一个无模式的SQL查询引擎。它是一种分布式查询处理语言。它适用于Hadoop、NoSQL和云存储。它的主要目的是低延迟的大规模数据处理。以下是Apache Drill的主要功能:</p>
<ol>
<li>能够扩展数千个节点。</li>
<li>支持NoSQL数据库，如Azure BLOB storage、Google Cloud Storage、亚马逊S3、HBase、MongoDB等</li>
<li>单个查询可以基于各种数据库。</li>
<li>支持数百万用户，并在大型数据集上为他们的查询提供服务。</li>
<li>提供更快的洞察力，没有加载、模式创建、维护、转换等ETL开销</li>
<li>分析多结构和嵌套数据，无需转换或过滤。</li>
</ol>
<h3 id="toc-14-apache-spark">14.阿帕奇火花</h3>
<p>它统一了各种大数据处理。Spark内置了用于流、SQL、机器学习和图形处理的库。Apache Spark为批处理和流处理提供了闪电般的性能。这是在DAG调度器、查询优化器和物理执行引擎的帮助下完成的。</p>
<ol>
<li>Spark可以在Hadoop、Mesos或Kubernetes上以独立集群模式运行。</li>
<li>Spark应用程序可以使用SQL、R、Python、Scala和Java编写。</li>
<li>Spark提供了80个高级操作符，这使得构建并行应用程序变得很容易。</li>
<li>它有各种各样的图书馆，比如</li>
<ol>
<li>机器学习的MLlib</li>
<li>用于图形处理的GraphX</li>
<li>SQL、数据帧和Spark流</li>
</ol>
<li>Spark执行内存处理，这使得它比Hadoop Map-Reduce更快。</li>
</ol>
<h3 id="toc-15-solr-lucene">15.索尔-卢斯内公司</h3>
<p>Apache Solr和Apache Lucene是搜索和索引Hadoop生态系统的两个服务。Apache Solr是围绕Apache Lucene构建的。Apache Lucene内置Java，使用Java库进行搜索和索引。Apache Solr是一个开源搜索平台。Apache Solr的不同特性如下:</p>
<ol>
<li>Solr具有高度的可伸缩性、可靠性和容错性。</li>
<li>它提供了</li>
<ol>
<li>分布式索引</li>
<li>自动化故障转移和恢复</li>
<li>负载平衡查询</li>
<li>集中式配置</li>
</ol>
<li>可以使用HTTP GET生成查询，并接收JSON、二进制、CSV和XML格式的结果。</li>
<li>它提供了匹配功能，如短语、通配符、分组、连接等等。</li>
<li>它有一个内置的管理界面，支持Solr实例的管理。</li>
<li>Solr利用了Lucene的实时索引。因此，它使用户能够在任何你想看的时候看到内容。</li>
</ol>
<p><a class="btn btn-primary btn-call-to-action btn-block" href="https://click.linksynergy.com/link?id=jU79Zysihs4&amp;offerid=1045023.996228&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fthe-ultimate-hands-on-hadoop-tame-your-big-data%2F" target="_blank" rel="noopener">Hadoop的终极实践:驯服您的大数据！</a></p>
<h2 id="conclusion">结论</h2>
<p>Hadoop生态系统的所有元素都是开放系统Apache Hadoop项目。</p>
<ol>
<li>核心是用于数据存储的HDFS、用于数据处理的Map-Reduce和作为资源管理器的YARN。</li>
<li>HIVE是一个数据分析工具</li>
<li>PIG是一种类似脚本语言的SQL。</li>
<li>h base–NoSQL数据库</li>
<li>一个机器学习工具</li>
<li>zookeeper——同步工具</li>
<li>oo zie–工作流调度系统</li>
<li>结构化数据导入和导出工具。</li>
<li>flume——一个用于非结构化和半结构化数据的数据传输工具</li>
<li>ambari——管理和保护Hadoop集群的工具</li>
</ol>
<p>一旦你清楚了以上的概念，你就可以认为你已经准备好进一步学习这个领域的知识了。</p>

									</div>

									</div>    
</body>
</html>