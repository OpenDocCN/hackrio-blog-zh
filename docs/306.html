<html>
<head>
<title>What is Apache Spark?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>什么是阿帕奇火花？</h1>
<blockquote>原文：<a href="https://hackr.io/blog/what-is-apache-spark#0001-01-01">https://hackr.io/blog/what-is-apache-spark#0001-01-01</a></blockquote><div><div class="content">
										<p>目前，Apache Spark是大规模批处理、机器学习、流处理和SQL操作的首选平台。自2009年发布以来，该平台在采用率和社区支持方面出现了大幅上升。</p>
<p>Apache Spark由1200多名开发人员构建，由300多家组织贡献，由供应商中立的Apache Software Foundation托管，是大数据处理领域最大的社区之一。</p>
<h2 id="what-is-apache-spark"><strong>什么是阿帕奇Spark？</strong></h2>
<p>Apache Spark是一个统一的大数据分析引擎。它用Scala编写，是一个开源的分布式集群计算框架。Spark提供了对整个集群(分布式计算机网络)进行编程的能力，具有隐式数据并行性和容错能力。</p>
<p>尽管Apache Spark作为一个独立的发行版免费提供，但对于那些对全面的企业管理服务感兴趣的人(通常遵循随用随付的方案)，它还提供了:</p>

<ul>
<li>亚马逊EMR(弹性MapReduce)</li>
<li>谷歌云数据平台</li>
<li>MapR</li>
<li>微软Azure HDInsight</li>
<li>来自Databricks的统一数据分析平台</li>
</ul>
<p>分布式大数据处理框架能够对庞大规模的数据集执行快速处理任务。它将数据处理任务分布在各个集群上，或者自己完成，或者与上面提到的其他分布式计算工具一起完成。</p>
<p>Apache Spark的这些品质不仅使它成为搅动大数据的理想工具，也是一个优秀的机器学习工具。组织通常使用Apache Spark来处理Brobdingnagian数据集，并得出有利可图的见解，以及构建可以做同样事情的健壮应用程序。</p>
<p>直观的Spark API抽象了大数据处理和分布式计算的复杂性，使开发人员可以根据自己的需求轻松使用分布式大数据处理框架，而不必担心过于深入其中的机制。</p>
<p>由于其通用性质，集群计算框架被不同细分市场的组织所采用。苹果、易贝、脸书、IBM、网飞和雅虎是利用Apache Spark的一些大公司。</p>
<p>所以，现在我们已经对Apache Spark有了一个简单的了解，是时候继续讨论Apache Spark架构了。</p>
<h3 id="the-apache-spark-architecture-what-and-how">Apache Spark架构-什么和如何？</h3>
<p>任何Apache Spark应用程序都由两个主要组件组成:</p>

<ul>
<li>驱动程序——将用户代码转换成几个任务，这些任务分布在不同的工作节点上。</li>
<li><strong> Executor进程</strong>——运行在worker节点上，执行分配给它们的任务。</li>
</ul>
<p>遵循分布式计算的原则，Apache Spark依赖于一个驱动程序核心进程，该进程将一个应用程序分成几个任务，并在执行子任务的几个执行器进程之间分配这些任务。</p>
<p>这些执行器的一个令人惊奇的能力是，它们可以根据它们所属的应用程序的需求进行缩放。</p>
<p>为了使这两个组件(驱动程序和执行器)保持同步，也为了使整个过程保持高效，即以最佳方式分配工作节点，需要一个资源或集群管理系统。</p>
<p>默认情况下，Apache Spark以独立集群模式运行。这要求集群中的每台机器都有Apache Spark框架和JVM。然而，这只是一个基本的集群管理器，还有更好的选项可供选择。</p>
<p>这就是为什么在企业场景中，Hadoop YARN被用作Apache Spark的集群管理系统的原因。然而，Spark也可以在Apache Mesos、Docker Swarm和Kubernetes上运行。</p>
<p>Apache Spark有一个DAG(有向无环图)调度器。它是集群计算框架的调度层。Spark将数据处理命令内置到DAG中。简而言之，它决定了任务和工作节点的内容和时间。</p>
<p><a class="btn btn-primary btn-call-to-action " href="https://click.linksynergy.com/deeplink?id=jU79Zysihs4&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fapache-spark-for-java-developers%2F" target="_blank" rel="noopener">面向Java开发人员的Apache Spark】</a></p>
<h2 id="the-apache-spark-ecosystem"><strong>Apache Spark生态系统</strong></h2>
<p>Apache Spark生态系统包括许多模块，我们将逐一讨论这些模块:</p>
<h3 id="toc-1-spark-core"><strong> 1。火花核心</strong></h3>
<p>Apache Spark中的一切都构建在Spark核心之上，Spark核心是集群计算框架的底层执行引擎。它的特点是:</p>
<ul>
<li>使用Java、Python和Scala APIs轻松开发</li>
<li>更快执行的内存计算能力，以及</li>
<li>通过通用的执行模型支持各种应用程序</li>
</ul>
<p>大多数Apache Spark核心API都是基于RDD(弹性分布式数据集)的概念构建的。虽然Spark内核简化了传统的map和reduce功能，但它还提供了对以下功能的内置支持:</p>
<ul>
<li>聚合，</li>
<li>过滤，</li>
<li>连接数据集，以及</li>
<li>抽样</li>
</ul>
<h3 id="toc-2-spark-sql-and-dataframes"><strong> 2。火花SQL和数据帧</strong></h3>
<p>Spark SQL -以前称为Shark - module，旨在用于结构化数据处理，充当分布式SQL查询引擎，并提供一个被称为DataFrames的编程抽象。</p>
<p>Spark SQL允许未修改的Hadoop Hive查询在现有数据和部署上运行得更快。它还提供了与Spark生态系统其余部分的强大集成选项，例如将SQL查询处理与机器学习相集成。</p>
<p>这个结构化数据处理模块提供了对类似SQL 2003的接口的支持，使开发人员和分析人员同样可以使用它。Spark SQL提供了开箱即用的标准接口，用于读取和写入:</p>
<ul>
<li>阿帕奇蜂房</li>
<li>阿帕奇兽人</li>
<li>阿帕奇拼花地板</li>
<li>HDFS</li>
<li>JDBC</li>
<li>JSON</li>
</ul>
<p>对其他流行数据存储的支持，如Apache Cassandra、Apache HBase和<a href="https://hackr.io/blog/what-is-mongodb-applications-advantages-examples"> MongoDB </a>，可以通过使用Spark包中不同的连接器来获得。一旦数据框注册为临时表，就可以在其上使用SQL查询。</p>
<p>在底层，Apache Spark利用Catalyst——一种查询优化器——负责检查数据和查询，为数据局部性和计算设计适当的查询计划，以便在整个集群中执行所需的计算。</p>
<p><strong> 3。火花RDDs </strong></p>
<p>RDDs代表弹性分布式数据集。它是一种编程抽象，表示不可变的对象集合，分布在整个计算集群中。</p>
<p>这些允许实现快速和可伸缩的并行处理，使用跨集群分割rdd上的操作的能力，然后在并行批处理中执行它们。可以从一系列实体中创建rdd，包括:</p>
<ul>
<li>亚马逊S3桶</li>
<li>像Apache Cassandra和MongoDB这样的NoSQL数据存储</li>
<li>纯文本文件</li>
<li>SQL数据库</li>
</ul>
<p><strong>注意</strong>:虽然Spark 2和更高版本仍然支持RDD接口，但是官方推荐使用Spark SQL模块。只有在Spark SQL模块无法满足项目的所有数据处理需求的情况下，您才应该使用RDD接口。</p>
<h3 id="toc-4-spark-graphx"><strong> 4。火花图X </strong></h3>
<p>Spark GraphX是一个图形计算引擎，允许用户交互式地构建、管理和转换图形结构的数据。它具有一系列用于处理图形结构的分布式算法，甚至包括Google PageRank的实现。</p>
<p>GraphX可用的算法利用了Spark Core的RDD数据建模方法。GraphFrames包允许在数据帧上进行图形操作，同时受益于图形查询的催化剂。</p>
<h3 id="toc-5-spark-streaming"><strong> 5。火花流</strong></h3>
<p>在Apache Hadoop的早期，批处理和流处理是两个独立的概念。对于批处理，需要编写MapReduce代码，而对于实时流，则需要使用Apache Storm之类的工具。</p>
<p>然而，现代应用不需要受限于分析和处理批量数据的能力。大多数甚至需要处理实时数据流。</p>
<p>经典Hadoop方法的一个主要缺点是不同的代码库需要保持同步。这是困难的，如果不是令人沮丧的话，因为技术所基于的框架、所需的资源、操作方法；一切都不同了。</p>
<p>为了解决这种看似矛盾的情况，Apache Spark提供了Spark流模块。它遵循Spark的易用性和容错标准，同时为需要处理存储和实时数据的应用程序提供强大的接口。</p>
<h3 id="toc-6-micro-batching"><strong> 6。微量配料</strong></h3>
<p>Spark流模块可以快速与一系列流行的数据源集成，如Flume、HDFS、Kafka，甚至Twitter。它将数据流分解成一系列连续的微批处理，之后可以使用Spark API对其进行操作。</p>

<p>这允许批处理和流形式的代码在相同的框架上运行时共享相同的代码。然而，微批处理被批评为在需要对传入数据进行低延迟响应的情况下速度很慢。</p>
<h3 id="toc-7-structured-streaming"><strong> 7。结构化流</strong></h3>
<p>其他面向流的框架，如Apache Apex和Apache Storm，由于使用了纯流方法，在上述场景中提供了更好的性能。这个问题通过使用结构化流解决了，这是Apache Spark 2.x版本中提供的一个特性。</p>
<p>高级API允许开发者创建无限的流数据帧和数据集。它还解决了几个紧迫的问题，如:</p>
<ul>
<li>事件时间聚合，以及</li>
<li>邮件传递延迟</li>
</ul>
<p>除了能够交互运行之外，对结构化流的每个查询都要经过Catalyst。这允许对实时流数据执行SQL查询。</p>
<h3 id="toc-8-continuous-processing"><strong> 8。连续加工</strong></h3>
<p>在Apache Spark 2.3之前，结构化流依赖于微批处理方案。Spark 2.3中增加了连续处理，这是一种低延迟的处理模式。令人印象深刻的是，它允许处理延迟低至1毫秒的响应。</p>
<p>连续流模式支持一组有限的查询，在Spark 2.4中仍被标记为实验性的。</p>
<h3 id="toc-9-spark-mlib"><strong> 9。火花MLib </strong></h3>
<p>MLib模块是Apache Spark生态系统中的一个可扩展的<a href="https://hackr.io/blog/best-machine-learning-libraries">机器学习库</a>，允许使用快速、高质量的ML算法。它可以包含在完整的工作流中，因为该库可以在Java、Python和Scala编程语言中使用。</p>
<p>借助Spark MLib，模块开发人员可以创建高效的机器学习管道。此外，他们可以轻松地在结构化数据集上实现特征提取、选择和转换。</p>
<p>MLib的特点是聚类和分类算法的分布式实现，例如k-means聚类和随机森林。这些可以很容易地添加到定制的ml管道中或从中删除。</p>
<p>数据科学家可以利用R或Python在Apache Spark中训练机器学习模型，然后可以导入到Java或Scala管道中，几乎即时投入生产使用。</p>

<p>然而，MLib只能用于基本的ml任务，即分类、聚类、过滤和回归。对于建模和训练深度神经网络，深度学习管道(仍在开发中)已经存在。</p>

<h3 id="toc-10-deep-learning-pipelines">10。深度学习管道</h3>
<p>Apache Spark使用深度学习管道提供对深度学习的支持。这些利用了现有的MLib管道结构，用于:</p>
<ul>
<li>将自定义Keras模型/张量流图应用于输入数据</li>
<li>调用构造分类器和低级深度学习库</li>
</ul>
<p>深度学习管道甚至允许将深度学习模型应用于可用数据，作为SQL语句的一部分。它通过将Keras模型和/或张量流图注册为自定义Spark SQL用户定义函数(UDF)来实现这一点。</p>
<h2 id="why-spark-and-not-hadoop"><strong>为什么是Spark而不是Hadoop？</strong></h2>
<p>Apache Spark和Apache Hadoop都是姐妹技术，算是吧。虽然两者有一些相似之处，本质上都是大数据处理平台，但一些不同之处使它们成为不同的大数据处理技术。</p>
<p>有趣的是，Spark包含在最近的许多Apache Hadoop发行版中。Hadoop因其MapReduce范式而脱颖而出。然而，Apache Spark有一个更好的替代方案。</p>
<p>在这个比较中，我们将基于两个主要参数来限制对两个Apache产品的比较；速度和开发者友好性。</p>
<h3 id="toc-1-speed"><strong> 1。速度</strong></h3>
<p>Apache Spark有一个内存数据引擎。这使得大数据处理框架能够更快地运行工作负载。在某些特定场景下，它可以比<a href="https://en.wikipedia.org/wiki/MapReduce"> MapReduce范式</a> (Apache Hadoop)快100倍。</p>
<p>例如，Spark对于需要在不同阶段之间向磁盘写入状态的多阶段作业来说是高性能的。数据不能完全包含在内存中的Apache Spark作业仍然比使用MapReduce技术运行的作业快10倍左右。</p>
<p>MapReduce处理技术仅限于包含数据映射和简化的两阶段执行图。另一方面，Spark的DAG(有向无环图)调度器具有多个阶段，能够更有效地分布。</p>
<h3 id="toc-2-developer-friendliness"><strong> 2。开发者友好型</strong></h3>
<p>Apache Spark的构建考虑到了开发人员友好性。除了为流行的数据分析编程语言如<a href="https://hackr.io/blog/r-vs-python"> R和Python </a>提供绑定，集群计算框架还为通用编程语言如Java和Scala提供绑定。</p>
<p>由于其易用性，Apache Spark提供的优势可以被任何人利用，从经验丰富的应用程序开发人员和专门的数据科学家到热情的学习者。使用Apache Spark可以用Java、Python、R、Scala和SQL快速编写应用程序。</p>

<p>集群计算框架包含80多个高级操作人员，有助于并行应用的开发。此外，Apache Spark可以通过Python、R、Scala和SQL shells交互使用。</p>

<p>综上所述，Apache Hadoop是一个老牌的大数据处理平台。同时，Apache Spark是一个更高效的现代大数据处理平台，扩展了其前身Hadoop的功能。</p>
<p>想深入了解Hadoop和Spark的区别？查看这个详细的<a href="https://hackr.io/blog/hadoop-vs-spark"> Hadoop与Spark </a>对比。</p>
<h2 id="summary"><strong>总结</strong></h2>
<p>Apache Spark是目前最流行的分布式大数据处理框架。尽管它有一个复杂的底层架构，但它被抽象为提供一个更简单的工作时间的方式确实是一个非凡的壮举。</p>
<p>对于任何对大数据处理和分布式计算着迷的人来说，<a href="https://hackr.io/tutorials/learn-apache-spark?ref=blog-post">学习Spark </a>无疑是有利的，而且是一个大大的肯定！</p>
<p>分布式集群计算框架与生俱来的用户友好性，加上巨大的、活跃的社区支持，使得理解和使用Apache Spark成为一项有益的、数字启发性的事业。所以，一切顺利！</p>
<p>你在准备Apache Spark的面试吗？查看这些<a href="https://hackr.io/blog/apache-spark-interview-questions">最佳Apache Spark面试问题</a>。</p>
<p><strong>人也在读:</strong></p>


									</div>

									</div>    
</body>
</html>